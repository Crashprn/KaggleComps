{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping, Checkpoint, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaled_data_by_col(df, min_max_cols, normalize_cols, y_cols, col_name, col):\n",
    "    if col_name in min_max_cols:\n",
    "        min_max_cols.remove(col_name)\n",
    "    if col_name in normalize_cols:\n",
    "        normalize_cols.remove(col_name)\n",
    "\n",
    "    db = df[df[col_name] == col]\n",
    "    db = db.drop(columns=[col_name])\n",
    "\n",
    "    x_min_max = db[min_max_cols].values.astype(np.float32)\n",
    "    x_normalize = db[normalize_cols].values.astype(np.float32)\n",
    "    y = db[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "    min_max_scaler = MinMaxScaler().fit(x_min_max)\n",
    "    normalize_scaler = StandardScaler().fit(x_normalize)\n",
    "    y_scaler = StandardScaler().fit(y)\n",
    "\n",
    "    x_min_max = min_max_scaler.transform(x_min_max)\n",
    "    x_normalize = normalize_scaler.transform(x_normalize)\n",
    "    y_final = y_scaler.transform(y)\n",
    "\n",
    "    db[min_max_cols] = x_min_max\n",
    "    db[normalize_cols] = x_normalize\n",
    "    db[y_cols] = y_final\n",
    "    \n",
    "\n",
    "    return (db, min_max_scaler, normalize_scaler, y_scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "store_nbr",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "family",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "onpromotion",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "city",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "state",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "store_type",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cluster",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "oil",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "h_type_nat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_description_nat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_transferred_nat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_type_loc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_description_loc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_transferred_loc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "month",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dow_avg_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_rolling_3_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_rolling_7_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_avg_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_rolling_3_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_rolling_7_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_7_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_14_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_7_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_14_transactions",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b9bd6f5e-521c-49ec-84b5-c8f139ad68dd",
       "rows": [
        [
         "0",
         "2013-02-01 00:00:00",
         "1",
         "0",
         "3.0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "97.46",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2013",
         "2",
         "1",
         "4",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "2013-02-01 00:00:00",
         "1",
         "1",
         "0.0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "97.46",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2013",
         "2",
         "1",
         "4",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "2013-02-01 00:00:00",
         "1",
         "2",
         "0.0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "97.46",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2013",
         "2",
         "1",
         "4",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "2013-02-01 00:00:00",
         "1",
         "3",
         "941.0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "97.46",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2013",
         "2",
         "1",
         "4",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "2013-02-01 00:00:00",
         "1",
         "4",
         "0.0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "97.46",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2013",
         "2",
         "1",
         "4",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 30,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>store_type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>oil</th>\n",
       "      <th>...</th>\n",
       "      <th>dow_avg_sales</th>\n",
       "      <th>dow_rolling_3_sales</th>\n",
       "      <th>dow_rolling_7_sales</th>\n",
       "      <th>dow_avg_transactions</th>\n",
       "      <th>dow_rolling_3_transactions</th>\n",
       "      <th>dow_rolling_7_transactions</th>\n",
       "      <th>rolling_7_sales</th>\n",
       "      <th>rolling_14_sales</th>\n",
       "      <th>rolling_7_transactions</th>\n",
       "      <th>rolling_14_transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>97.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>97.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>97.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>941.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>97.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>97.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_nbr  family  sales  onpromotion  city  state  store_type  \\\n",
       "0 2013-02-01          1       0    3.0            0     0      0           0   \n",
       "1 2013-02-01          1       1    0.0            0     0      0           0   \n",
       "2 2013-02-01          1       2    0.0            0     0      0           0   \n",
       "3 2013-02-01          1       3  941.0            0     0      0           0   \n",
       "4 2013-02-01          1       4    0.0            0     0      0           0   \n",
       "\n",
       "   cluster    oil  ...  dow_avg_sales  dow_rolling_3_sales  \\\n",
       "0       13  97.46  ...            0.0                  0.0   \n",
       "1       13  97.46  ...            0.0                  0.0   \n",
       "2       13  97.46  ...            0.0                  0.0   \n",
       "3       13  97.46  ...            0.0                  0.0   \n",
       "4       13  97.46  ...            0.0                  0.0   \n",
       "\n",
       "   dow_rolling_7_sales  dow_avg_transactions  dow_rolling_3_transactions  \\\n",
       "0                  0.0                   0.0                         0.0   \n",
       "1                  0.0                   0.0                         0.0   \n",
       "2                  0.0                   0.0                         0.0   \n",
       "3                  0.0                   0.0                         0.0   \n",
       "4                  0.0                   0.0                         0.0   \n",
       "\n",
       "   dow_rolling_7_transactions  rolling_7_sales  rolling_14_sales  \\\n",
       "0                         0.0              0.0               0.0   \n",
       "1                         0.0              0.0               0.0   \n",
       "2                         0.0              0.0               0.0   \n",
       "3                         0.0              0.0               0.0   \n",
       "4                         0.0              0.0               0.0   \n",
       "\n",
       "   rolling_7_transactions  rolling_14_transactions  \n",
       "0                     0.0                      0.0  \n",
       "1                     0.0                      0.0  \n",
       "2                     0.0                      0.0  \n",
       "3                     0.0                      0.0  \n",
       "4                     0.0                      0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = 'data/'\n",
    "df = pd.read_csv(os.path.join(data_dir, 'train_data.csv'))\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(by=['date', 'store_nbr'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['store_nbr', 'city', 'state', 'store_type', 'cluster', 'h_type_nat', 'h_description_nat', 'h_transferred_nat', 'h_type_loc', 'h_description_loc', 'h_transferred_loc', 'month', 'day', 'day_of_week']\n",
      "Index(['date', 'store_nbr', 'family', 'sales', 'onpromotion', 'city', 'state',\n",
      "       'store_type', 'cluster', 'oil', 'h_type_nat', 'h_description_nat',\n",
      "       'h_transferred_nat', 'h_type_loc', 'h_description_loc',\n",
      "       'h_transferred_loc', 'year', 'month', 'day', 'day_of_week',\n",
      "       'dow_avg_sales', 'dow_rolling_3_sales', 'dow_rolling_7_sales',\n",
      "       'dow_avg_transactions', 'dow_rolling_3_transactions',\n",
      "       'dow_rolling_7_transactions', 'rolling_7_sales', 'rolling_14_sales',\n",
      "       'rolling_7_transactions', 'rolling_14_transactions'],\n",
      "      dtype='object')\n",
      "rows_before 2910006\n",
      "rows_after 35640\n",
      "rows_total 2945646\n",
      "2017-07-26 00:00:00\n",
      "2017-07-27 00:00:00\n"
     ]
    }
   ],
   "source": [
    "min_max_cols = ['store_nbr', 'city', 'state', 'store_type', 'cluster', 'h_type_nat', 'h_description_nat', 'h_transferred_nat', 'h_type_loc', 'h_description_loc', 'h_transferred_loc', 'month', 'day', 'day_of_week']\n",
    "normalize_cols = ['onpromotion', 'oil', 'dow_avg_sales', 'dow_rolling_3_sales', 'rolling_7_sales', 'rolling_14_sales', 'dow_avg_transactions', 'dow_rolling_3_transactions', 'rolling_7_transactions', 'rolling_14_transactions']\n",
    "x_cols = min_max_cols + normalize_cols\n",
    "y_cols = ['sales']\n",
    "split_col = 'family'\n",
    "\n",
    "print(min_max_cols)\n",
    "print(df.columns)\n",
    "\n",
    "final_run = False\n",
    "\n",
    "if final_run:\n",
    "    train_df = df\n",
    "else:\n",
    "    rows_before = (df['date'] < '2017-07-27')\n",
    "    rows_after = ~rows_before\n",
    "\n",
    "    print('rows_before', rows_before.sum())\n",
    "    print('rows_after', rows_after.sum())\n",
    "    print('rows_total', len(df))\n",
    "\n",
    "    train_df = df[rows_before]\n",
    "    val_df = df[rows_after]\n",
    "\n",
    "    print(train_df['date'].max())\n",
    "    print(val_df['date'].min())\n",
    "\n",
    "train_df_by_cluster = {}\n",
    "scaler_x_by_cluster = {}\n",
    "scaler_y_by_cluster = {}\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    cluster_df, cluster_min_max_scaler, cluster_normalize_scaler, cluster_y_scaler = create_scaled_data_by_col(train_df, min_max_cols, normalize_cols, y_cols, split_col, cluster)\n",
    "    train_df_by_cluster[cluster] = cluster_df\n",
    "    scaler_x_by_cluster[cluster] = (cluster_min_max_scaler, cluster_normalize_scaler)\n",
    "    scaler_y_by_cluster[cluster] = cluster_y_scaler\n",
    "\n",
    "if not final_run:\n",
    "    val_df_by_cluster = {}\n",
    "\n",
    "    for cluster in df[split_col].unique():\n",
    "        val_cluster_min_max_scaler, val_cluster_normalize_scaler = scaler_x_by_cluster[cluster]\n",
    "        val_cluster_y_scaler = scaler_y_by_cluster[cluster]\n",
    "\n",
    "        val_cluster_df = val_df[val_df[split_col] == cluster]\n",
    "        val_cluster_df = val_cluster_df.drop(columns=split_col)\n",
    "\n",
    "        val_cluster_x_min_max = val_cluster_df[min_max_cols].values.astype(np.float32)\n",
    "        val_cluster_x_normalize = val_cluster_df[normalize_cols].values.astype(np.float32)\n",
    "        val_cluster_y = val_cluster_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "        val_cluster_x_min_max = val_cluster_min_max_scaler.transform(val_cluster_x_min_max)\n",
    "        val_cluster_x_normalize = val_cluster_normalize_scaler.transform(val_cluster_x_normalize)\n",
    "        val_cluster_y = val_cluster_y_scaler.transform(val_cluster_y)\n",
    "\n",
    "        val_cluster_df[min_max_cols] = val_cluster_x_min_max\n",
    "        val_cluster_df[normalize_cols] = val_cluster_x_normalize\n",
    "        val_cluster_df[y_cols] = val_cluster_y\n",
    "\n",
    "        val_df_by_cluster[cluster] = val_cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FFNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim=64, num_hidden_layers=2):\n",
    "        super(FFNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        self.head = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_hidden_layers):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim ))\n",
    "\n",
    "        self.stem = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.head(x))\n",
    "\n",
    "        for layer in self.stem:\n",
    "            x =  x + self.act(layer(x))\n",
    "\n",
    "        return self.output(x)\n",
    "\n",
    "class LSTMNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, endogenous_dim, endogenous_len, exogenous_dim, hidden_dim, out_dim, out_seq_len, num_layers):\n",
    "        super(LSTMNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.endogenous_dim = endogenous_dim\n",
    "        self.exogenous_dim = exogenous_dim\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        self.linear_proj = nn.Linear(endogenous_dim* endogenous_len, (input_dim - exogenous_dim) * out_seq_len)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_linear = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        endogenous = x[0]\n",
    "        exogenous = x[1]\n",
    "        out = self.act(self.linear_proj(endogenous))\n",
    "\n",
    "        out = out.view(-1, self.out_seq_len, self.input_dim - self.exogenous_dim)\n",
    "        out = torch.cat((out, exogenous), dim=-1)\n",
    "\n",
    "        out, _ = self.lstm(out)\n",
    "\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, data, endogenous_len, out_seq_len, index_col, endogenous_cols, exogenous_cols, out_cols, add_order=None):\n",
    "        self.data = data.sort_values(by=[index_col, add_order] if add_order is not None else [index_col]) \n",
    "        self.add_len = data[add_order].nunique() if add_order is not None else 0\n",
    "        self.add_cols = add_order if add_order is not None else None\n",
    "        self.endogenous_len = endogenous_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.index_col = index_col\n",
    "        self.endogenous_cols = endogenous_cols\n",
    "        self.exogenous_cols = exogenous_cols\n",
    "        self.out_cols = out_cols\n",
    "        self.indices = {i:j for i,j in enumerate(self.data[self.index_col].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) - self.endogenous_len - self.out_seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        date_in = self.indices[idx]\n",
    "        date_mid = self.indices[idx + self.endogenous_len -1]\n",
    "        date_end = self.indices[idx + self.endogenous_len + self.out_seq_len -1]\n",
    "\n",
    "        in_rows = (self.data[self.index_col] >= date_in) & (self.data[self.index_col] <= date_mid)\n",
    "        out_rows = (self.data[self.index_col] > date_mid) & (self.data[self.index_col] <= date_end)\n",
    "\n",
    "        endog = self.data[in_rows][self.endogenous_cols].values.reshape(-1)\n",
    "        exog = self.data[out_rows][self.exogenous_cols]\n",
    "        if self.add_cols is not None:\n",
    "            exog = exog.iloc[[self.add_len * i for i in range(self.out_seq_len)]]\n",
    "            exog = exog.values.reshape(self.out_seq_len, len(self.exogenous_cols))\n",
    "\n",
    "\n",
    "        y = self.data[out_rows][self.out_cols].values\n",
    "        y = y.reshape(self.out_seq_len, self.add_len)\n",
    "\n",
    "        sample = {\n",
    "            'endog': endog,\n",
    "            'exog': exog,\n",
    "            'label': y\n",
    "        }\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_by_cluster = {}\n",
    "train_params = {\n",
    "                \"criterion\": nn.L1Loss,\n",
    "                \"optimizer\": torch.optim.AdamW,\n",
    "                \"optimizer__weight_decay\": 1e-8,\n",
    "                #'train_split' : None,\n",
    "                #\"train_split\": predefined_split(Dataset(val_x, val_y)),\n",
    "                \"lr\": 0.001,\n",
    "                \"batch_size\": 128,\n",
    "                \"max_epochs\": 1000,\n",
    "                \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                \"iterator_train__shuffle\": False,\n",
    "                \"iterator_train__num_workers\": 2,\n",
    "                \"iterator_train__pin_memory\": True,\n",
    "                \"iterator_valid__shuffle\": False,\n",
    "                \"iterator_valid__num_workers\": 2,\n",
    "                \"iterator_valid__pin_memory\": True,\n",
    "                \"verbose\": 2,\n",
    "        }\n",
    "\n",
    "net_params = {\n",
    "    'input_dim': len(x_cols),\n",
    "    'out_dim': 1,\n",
    "    'hidden_dim': 200,\n",
    "    'num_hidden_layers': 6,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4535\u001b[0m        \u001b[32m0.4655\u001b[0m     +  1.4670\n",
      "      2        \u001b[36m0.4384\u001b[0m        \u001b[32m0.4592\u001b[0m     +  1.3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-37 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/codygrogan/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib64/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/codygrogan/.local/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 53, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/codygrogan/.local/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 30, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codygrogan/.local/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 495, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/multiprocessing/connection.py\", line 525, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/usr/lib64/python3.12/multiprocessing/connection.py\", line 962, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/multiprocessing/connection.py\", line 399, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.1979\u001b[0m        \u001b[32m0.2744\u001b[0m     +  1.2951\n",
      "      2        \u001b[36m0.1747\u001b[0m        \u001b[32m0.2719\u001b[0m     +  1.2688\n",
      "      3        \u001b[36m0.1688\u001b[0m        \u001b[32m0.2716\u001b[0m     +  1.2690\n",
      "      4        \u001b[36m0.1669\u001b[0m        \u001b[32m0.2712\u001b[0m     +  1.2912\n",
      "      5        \u001b[36m0.1640\u001b[0m        \u001b[32m0.2703\u001b[0m     +  1.2950\n",
      "      6        \u001b[36m0.1634\u001b[0m        \u001b[32m0.2695\u001b[0m     +  1.2497\n",
      "      7        \u001b[36m0.1630\u001b[0m        \u001b[32m0.2694\u001b[0m     +  1.2850\n",
      "      8        \u001b[36m0.1629\u001b[0m        \u001b[32m0.2679\u001b[0m     +  1.3391\n",
      "      9        \u001b[36m0.1620\u001b[0m        \u001b[32m0.2665\u001b[0m     +  1.3034\n"
     ]
    }
   ],
   "source": [
    "for cluster in df[split_col].unique():\n",
    "    train_df = train_df_by_cluster[cluster]\n",
    "    train_x = train_df[x_cols].values.astype(np.float32)\n",
    "    train_y = train_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "    if not final_run:\n",
    "        val_df = val_df_by_cluster[cluster]\n",
    "        train_params['train_split'] = predefined_split(Dataset(val_df[x_cols].values.astype(np.float32), val_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)))\n",
    "    else:\n",
    "        train_params['train_split'] = None\n",
    "\n",
    "    callbacks = [EarlyStopping(patience=15, threshold=0.001, threshold_mode='abs', monitor='valid_loss', lower_is_better=True),\n",
    "            Checkpoint(monitor='valid_loss_best', f_params=f'sales_forecaster_{cluster}.pt', dirname='models/'),\n",
    "            LRScheduler(policy=ReduceLROnPlateau, monitor='valid_loss', factor=0.5, patience=5, threshold=0.001, threshold_mode='abs', mode='min', verbose=True)\n",
    "            ]\n",
    "\n",
    "    train_params['callbacks'] = callbacks\n",
    "\n",
    "    net = NeuralNetRegressor(FFNeuralNetwork(**net_params), **train_params)\n",
    "\n",
    "    net.fit(train_x, train_y)\n",
    "    net_by_cluster[cluster] = net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Nets from Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in df[split_col].unique():\n",
    "    net = NeuralNetRegressor(FFNeuralNetwork(**net_params), **train_params)\n",
    "    net.initialize()\n",
    "    net.load_params(f_params=f'models/sales_forecaster_{cluster}.pt')\n",
    "    net_by_cluster[cluster] = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'store_nbr', 'sales', 'onpromotion', 'city', 'state',\n",
      "       'store_type', 'cluster', 'oil', 'h_type_nat', 'h_description_nat',\n",
      "       'h_transferred_nat', 'h_type_loc', 'h_description_loc',\n",
      "       'h_transferred_loc', 'year', 'month', 'day', 'day_of_week',\n",
      "       'dow_avg_sales', 'dow_rolling_3_sales', 'dow_rolling_7_sales',\n",
      "       'dow_avg_transactions', 'dow_rolling_3_transactions',\n",
      "       'dow_rolling_7_transactions', 'rolling_7_sales', 'rolling_14_sales',\n",
      "       'rolling_7_transactions', 'rolling_14_transactions'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df_by_cluster[cluster].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "endogenous_cols = [\n",
    "        'sales', 'onpromotion', 'oil', \n",
    "       'dow_avg_sales', 'dow_rolling_3_sales', 'dow_rolling_7_sales',\n",
    "       'dow_avg_transactions', 'dow_rolling_3_transactions',\n",
    "       'dow_rolling_7_transactions', 'rolling_7_sales', 'rolling_14_sales',\n",
    "       'rolling_7_transactions', 'rolling_14_transactions']\n",
    "\n",
    "exogenous_cols = [\n",
    "    'h_type_nat', 'h_description_nat', 'h_transferred_nat', 'h_type_loc',\n",
    "    'h_description_loc', 'h_transferred_loc', 'month', 'day', 'day_of_week', 'store_nbr'\n",
    "    ]\n",
    "\n",
    "out_cols = ['sales']\n",
    "\n",
    "\n",
    "lstm_net_by_cluster = {}\n",
    "lstm_net_params = {\n",
    "    'input_dim': 256,\n",
    "    'endogenous_dim': len(endogenous_cols)*54,\n",
    "    'endogenous_len': 5,\n",
    "    'exogenous_dim': len(exogenous_cols),\n",
    "    'hidden_dim': 512,\n",
    "    'out_dim': 54,\n",
    "    'out_seq_len': 15,\n",
    "    'num_layers': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Val Loss: 0.5246623754501343\n",
      "Epoch 2/1000, Val Loss: 0.5180840492248535\n",
      "Epoch 3/1000, Val Loss: 0.5258320569992065\n",
      "Epoch 4/1000, Val Loss: 0.5234028697013855\n",
      "Epoch 5/1000, Val Loss: 0.5300126075744629\n",
      "Epoch 6/1000, Val Loss: 0.530662477016449\n",
      "Epoch 7/1000, Val Loss: 0.5329450964927673\n",
      "Epoch 8/1000, Val Loss: 0.531719446182251\n",
      "Epoch 9/1000, Val Loss: 0.5411142706871033\n",
      "Epoch 10/1000, Val Loss: 0.5426766872406006\n",
      "Epoch 11/1000, Val Loss: 0.5436486005783081\n",
      "Epoch 12/1000, Val Loss: 0.5435389876365662\n",
      "Epoch 13/1000, Val Loss: 0.5330765843391418\n",
      "Epoch 14/1000, Val Loss: 0.5339668989181519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m val_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     26\u001b[0m         endog \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendog\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     27\u001b[0m         exog \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexog\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[20], line 86\u001b[0m, in \u001b[0;36mLSTMDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     82\u001b[0m     exog \u001b[38;5;241m=\u001b[39m exog\u001b[38;5;241m.\u001b[39miloc[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_len \u001b[38;5;241m*\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_seq_len)]]\n\u001b[1;32m     83\u001b[0m     exog \u001b[38;5;241m=\u001b[39m exog\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_seq_len, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexogenous_cols))\n\u001b[0;32m---> 86\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mout_rows\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_cols]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     87\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_len)\n\u001b[1;32m     89\u001b[0m sample \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendog\u001b[39m\u001b[38;5;124m'\u001b[39m: endog,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexog\u001b[39m\u001b[38;5;124m'\u001b[39m: exog,\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: y\n\u001b[1;32m     93\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:3752\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3750\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m   3751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 3752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3754\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[1;32m   3755\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[1;32m   3756\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3810\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 3811\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/generic.py:3948\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   3941\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   3943\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3946\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3947\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3948\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3949\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   3950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/generic.py:3932\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3925\u001b[0m         axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3926\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3927\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[1;32m   3928\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_range_indexer(indices, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m   3929\u001b[0m     ):\n\u001b[1;32m   3930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 3932\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3934\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3938\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/managers.py:963\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[1;32m    960\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    962\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/managers.py:748\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    740\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    741\u001b[0m         indexer,\n\u001b[1;32m    742\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    743\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[1;32m    744\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[1;32m    745\u001b[0m     )\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 748\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    756\u001b[0m     ]\n\u001b[1;32m    758\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    759\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/blocks.py:945\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m    942\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/array_algos/take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[1;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 1000\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    train_df = train_df_by_cluster[cluster]\n",
    "    train_dataset = LSTMDataset(train_df, 5, 15, 'date', endogenous_cols, exogenous_cols, out_cols, 'store_nbr')\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "    val_df = val_df_by_cluster[cluster]\n",
    "    val_dataset = LSTMDataset(val_df, 5, 15, 'date', endogenous_cols, exogenous_cols, out_cols, 'store_nbr')\n",
    "\n",
    "    model = LSTMNeuralNetwork(**lstm_net_params)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.L1Loss()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    model.zero_grad()\n",
    "\n",
    "    val = val_dataset[0]\n",
    "    val_endog = torch.tensor(val['endog']).unsqueeze(0).to(device).to(torch.float32)\n",
    "    val_exog = torch.tensor(val['exog']).unsqueeze(0).to(device).to(torch.float32)\n",
    "    val_y = torch.tensor(val['label']).unsqueeze(0).to(device).to(torch.float32)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for sample in train_loader:\n",
    "            endog = sample['endog'].to(device).to(torch.float32)\n",
    "            exog = sample['exog'].to(device).to(torch.float32)\n",
    "            y = sample['label'].to(device).to(torch.float32)\n",
    "            optim.zero_grad()\n",
    "            output = model((endog, exog))\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_pred = model((val_endog, val_exog))\n",
    "\n",
    "            val_loss = criterion(val_pred, val_y)\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss.item()}')\n",
    "            model.train()\n",
    "\n",
    "\n",
    "    lstm_net_by_cluster[cluster] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "cluster_rfs = {}\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    train_df = train_df_by_cluster[cluster]\n",
    "\n",
    "    train_x = train_df[x_cols].values.astype(np.float32)\n",
    "    train_y = train_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=4)\n",
    "    rf.fit(train_x, train_y.squeeze())\n",
    "\n",
    "    cluster_rfs[cluster] = rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "cluster_xgb = {}\n",
    "for cluster in df[split_col].unique():\n",
    "    train_x = train_x_by_cluster[cluster]\n",
    "    train_y = train_y_by_cluster[cluster]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=1000, max_depth=12, learning_rate=0.001, random_state=42, n_jobs=2)\n",
    "    xgb_model.fit(train_x, train_y.squeeze())\n",
    "\n",
    "    cluster_xgb[cluster] = xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_train_preds = []\n",
    "rf_train_preds = []\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    train_df = train_df_by_cluster[cluster]\n",
    "\n",
    "    train_x = train_df[x_cols].values.astype(np.float32)\n",
    "    train_y = train_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "    y_scaler = scaler_y_by_cluster[cluster]\n",
    "\n",
    "    net = net_by_cluster[cluster]\n",
    "    rf = cluster_rfs[cluster]\n",
    "\n",
    "    net_preds = net.predict(train_x)\n",
    "    rf_preds = rf.predict(train_x)\n",
    "\n",
    "    train_df['sales_nn'] = net_preds\n",
    "    train_df['sales_rf'] = rf_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loss Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_rfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m val_x \u001b[38;5;241m=\u001b[39m val_cluster_df[x_cols]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     12\u001b[0m val_y \u001b[38;5;241m=\u001b[39m val_cluster_df[y_cols]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(y_cols))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 14\u001b[0m rf \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_rfs\u001b[49m[cluster]\n\u001b[1;32m     15\u001b[0m net \u001b[38;5;241m=\u001b[39m net_by_cluster[cluster]\n\u001b[1;32m     17\u001b[0m rf_preds\u001b[38;5;241m.\u001b[39mappend(scaler_y_by_cluster[cluster]\u001b[38;5;241m.\u001b[39minverse_transform(rf\u001b[38;5;241m.\u001b[39mpredict(val_x)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_rfs' is not defined"
     ]
    }
   ],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_true) - np.log1p(y_pred))))\n",
    "\n",
    "rf_preds = []\n",
    "net_preds = []\n",
    "xgb_preds = []\n",
    "val_y_true = []\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    val_cluster_df = val_df_by_cluster[cluster]\n",
    "    val_x = val_cluster_df[x_cols].values.astype(np.float32)\n",
    "    val_y = val_cluster_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "    rf = cluster_rfs[cluster]\n",
    "    net = net_by_cluster[cluster]\n",
    "\n",
    "    rf_preds.append(scaler_y_by_cluster[cluster].inverse_transform(rf.predict(val_x).reshape(-1, 1)))\n",
    "    net_preds.append(scaler_y_by_cluster[cluster].inverse_transform(net.predict(val_x).reshape(-1, 1)).clip(0))    \n",
    "    #xgb_preds.append(scaler_y_by_cluster[cluster].inverse_transform(cluster_xgb[cluster].predict(val_x).reshape(-1, 1)))\n",
    "    val_y_true.append(scaler_y_by_cluster[cluster].inverse_transform(val_y))\n",
    "\n",
    "rf_preds = np.concatenate(rf_preds)\n",
    "net_preds = np.concatenate(net_preds)\n",
    "#xgb_preds = np.concatenate(xgb_preds)\n",
    "val_y_true = np.concatenate(val_y_true)\n",
    "\n",
    "print(f'RF RMSLE: {rmsle(val_y_true, rf_preds)}')\n",
    "#print(f'XGB RMSLE: {rmsle(val_y_true, xgb_preds)}')\n",
    "print(f'NN RMSLE: {rmsle(val_y_true, net_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "store_nbr",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "family",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "onpromotion",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "city",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "state",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "store_type",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cluster",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "oil",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "h_type",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_locale",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_locale_name",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "transferred",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "holiday_count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "month",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dow_avg_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_7_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_14_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_30_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_avg_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_7_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_14_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_30_transactions",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1bb27a0d-6c5e-49c8-9a95-db9e72a9a783",
       "rows": [
        [
         "3000888",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "3.5313807531380754",
         "3.4285714285714284",
         "3.928571428571429",
         "4.133333333333334",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ],
        [
         "3000889",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ],
        [
         "3000890",
         "1",
         "2",
         "2",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "2.6150627615062763",
         "3.857142857142857",
         "4.428571428571429",
         "3.7",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ],
        [
         "3000891",
         "1",
         "3",
         "20",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "1845.4853556485357",
         "2456.1428571428573",
         "2471.5714285714284",
         "2504.4666666666667",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ],
        [
         "3000892",
         "1",
         "4",
         "0",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "0.1673640167364016",
         "0.0",
         "0.5714285714285714",
         "0.7333333333333333",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>store_type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>oil</th>\n",
       "      <th>h_type</th>\n",
       "      <th>h_locale</th>\n",
       "      <th>...</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>dow_avg_sales</th>\n",
       "      <th>rolling_7_sales</th>\n",
       "      <th>rolling_14_sales</th>\n",
       "      <th>rolling_30_sales</th>\n",
       "      <th>dow_avg_transactions</th>\n",
       "      <th>rolling_7_transactions</th>\n",
       "      <th>rolling_14_transactions</th>\n",
       "      <th>rolling_30_transactions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3000888</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>3.531381</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000890</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2.615063</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000891</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1845.485356</td>\n",
       "      <td>2456.142857</td>\n",
       "      <td>2471.571429</td>\n",
       "      <td>2504.466667</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000892</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.167364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         store_nbr  family  onpromotion  city  state  store_type  cluster  \\\n",
       "id                                                                          \n",
       "3000888          1       0            0     0      0           0       13   \n",
       "3000889          1       1            0     0      0           0       13   \n",
       "3000890          1       2            2     0      0           0       13   \n",
       "3000891          1       3           20     0      0           0       13   \n",
       "3000892          1       4            0     0      0           0       13   \n",
       "\n",
       "          oil  h_type  h_locale  ...  day  day_of_week  dow_avg_sales  \\\n",
       "id                               ...                                    \n",
       "3000888  46.8       1         1  ...   16            2       3.531381   \n",
       "3000889  46.8       1         1  ...   16            2       0.000000   \n",
       "3000890  46.8       1         1  ...   16            2       2.615063   \n",
       "3000891  46.8       1         1  ...   16            2    1845.485356   \n",
       "3000892  46.8       1         1  ...   16            2       0.167364   \n",
       "\n",
       "         rolling_7_sales  rolling_14_sales  rolling_30_sales  \\\n",
       "id                                                             \n",
       "3000888         3.428571          3.928571          4.133333   \n",
       "3000889         0.000000          0.000000          0.000000   \n",
       "3000890         3.857143          4.428571          3.700000   \n",
       "3000891      2456.142857       2471.571429       2504.466667   \n",
       "3000892         0.000000          0.571429          0.733333   \n",
       "\n",
       "         dow_avg_transactions  rolling_7_transactions  \\\n",
       "id                                                      \n",
       "3000888           1863.393305             1888.857143   \n",
       "3000889           1863.393305             1888.857143   \n",
       "3000890           1863.393305             1888.857143   \n",
       "3000891           1863.393305             1888.857143   \n",
       "3000892           1863.393305             1888.857143   \n",
       "\n",
       "         rolling_14_transactions  rolling_30_transactions  \n",
       "id                                                         \n",
       "3000888              1863.857143                   1859.3  \n",
       "3000889              1863.857143                   1859.3  \n",
       "3000890              1863.857143                   1859.3  \n",
       "3000891              1863.857143                   1859.3  \n",
       "3000892              1863.857143                   1859.3  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sales",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "179f5508-6041-4eb4-a348-88e57feda975",
       "rows": [
        [
         "3000888",
         "3.8044147"
        ],
        [
         "3000889",
         "0.0005653285"
        ],
        [
         "3000890",
         "5.2990756"
        ],
        [
         "3000891",
         "2225.0566"
        ],
        [
         "3000892",
         "5.9548056e-05"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3000888</th>\n",
       "      <td>3.804415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000889</th>\n",
       "      <td>0.000565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000890</th>\n",
       "      <td>5.299076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000891</th>\n",
       "      <td>2225.056641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000892</th>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sales\n",
       "id                  \n",
       "3000888     3.804415\n",
       "3000889     0.000565\n",
       "3000890     5.299076\n",
       "3000891  2225.056641\n",
       "3000892     0.000060"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(data_dir, 'test_data.csv'), index_col=0)\n",
    "display(test_df.head())\n",
    "\n",
    "test_x_by_cluster = {}\n",
    "test_id_by_cluster = {}\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    test_cluster_min_max_scaler, test_cluster_normalize_scaler = scaler_x_by_cluster[cluster]\n",
    "    test_cluster_y_scaler = scaler_y_by_cluster[cluster]\n",
    "\n",
    "    test_cluster_x_df = test_df[test_df[split_col] == cluster]\n",
    "    test_cluster_x_df = test_cluster_x_df.drop(columns=split_col)\n",
    "\n",
    "    test_cluster_x_min_max = test_cluster_x_df[min_max_cols].values.astype(np.float32)\n",
    "    test_cluster_x_normalize = test_cluster_x_df[normalize_cols].values.astype(np.float32)\n",
    "\n",
    "    test_cluster_x_min_max = test_cluster_min_max_scaler.transform(test_cluster_x_min_max)\n",
    "    test_cluster_x_normalize = test_cluster_normalize_scaler.transform(test_cluster_x_normalize)\n",
    "\n",
    "    test_x_by_cluster[cluster] = np.concatenate([test_cluster_x_min_max, test_cluster_x_normalize], axis=1)\n",
    "    test_id_by_cluster[cluster] = test_cluster_x_df.index\n",
    "\n",
    "\n",
    "test_preds_dfs = []\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    test_x = test_x_by_cluster[cluster]\n",
    "    id = test_id_by_cluster[cluster]\n",
    "    #rf = cluster_rfs[cluster]\n",
    "\n",
    "    #pred_rf = scaler_y_by_cluster[cluster].inverse_transform(rf.predict(test_x).reshape(-1, 1))\n",
    "    #pred_xgb = scaler_y_by_cluster[cluster].inverse_transform(cluster_xgb[cluster].predict(test_x).reshape(-1, 1))\n",
    "    pred_nn = scaler_y_by_cluster[cluster].inverse_transform(net_by_cluster[cluster].predict(test_x).reshape(-1, 1)).clip(0)\n",
    "    \n",
    "    cluster_df = pd.DataFrame(np.concatenate([pred_nn], axis=1), index=id, columns=['sales_nn'])\n",
    "    #cluster_df = pd.DataFrame(np.concatenate([pred_rf, pred_nn], axis=1), index=id, columns=['sales_rf', 'sales_nn'])\n",
    "\n",
    "    test_preds_dfs.append(cluster_df)\n",
    "\n",
    "test_preds_df = pd.concat(test_preds_dfs)\n",
    "\n",
    "test_df = test_df.merge(test_preds_df, on='id', how='left')\n",
    "\n",
    "sub_df_nn = test_df[['sales_nn']]\n",
    "#sub_df_rf = test_df[['sales_rf']]\n",
    "#sub_df_xgb = test_df[['sales_xgb']]\n",
    "\n",
    "#sub_df_rf = sub_df_rf.rename(columns={'sales_rf': 'sales'})\n",
    "#sub_df_xgb = sub_df_xgb.rename(columns={'sales_xgb': 'sales'})\n",
    "sub_df_nn = sub_df_nn.rename(columns={'sales_nn': 'sales'})\n",
    "\n",
    "\n",
    "display(sub_df_nn.head())\n",
    "#display(sub_df_rf.head())\n",
    "#display(sub_df_xgb.head())\n",
    "\n",
    "sub_df_nn.to_csv('data/submission_nn.csv')\n",
    "#sub_df_xgb.to_csv('data/submission_xgb.csv')\n",
    "#sub_df_rf.to_csv('data/submission_rf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
