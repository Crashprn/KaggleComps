{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping, Checkpoint, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaled_data_by_col(df, min_max_cols, normalize_cols, y_cols, col_name, col):\n",
    "    if col_name in min_max_cols:\n",
    "        min_max_cols.remove(col_name)\n",
    "    if col_name in normalize_cols:\n",
    "        normalize_cols.remove(col_name)\n",
    "\n",
    "    db = df[df[col_name] == col]\n",
    "    db = db.drop(columns=[col_name])\n",
    "\n",
    "    x_min_max = db[min_max_cols].values.astype(np.float32)\n",
    "    x_normalize = db[normalize_cols].values.astype(np.float32)\n",
    "    y = db[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "    min_max_scaler = MinMaxScaler().fit(x_min_max)\n",
    "    normalize_scaler = StandardScaler().fit(x_normalize)\n",
    "    y_scaler = StandardScaler().fit(y)\n",
    "\n",
    "    x_min_max = min_max_scaler.transform(x_min_max)\n",
    "    x_normalize = normalize_scaler.transform(x_normalize)\n",
    "    y_final = y_scaler.transform(y)\n",
    "\n",
    "    db[min_max_cols] = x_min_max\n",
    "    db[normalize_cols] = x_normalize\n",
    "    db[y_cols] = y_final\n",
    "    \n",
    "\n",
    "    return (db, min_max_scaler, normalize_scaler, y_scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "store_nbr",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "family",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "onpromotion",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "city",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "state",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "store_type",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cluster",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "oil",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "h_type_nat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_description_nat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_transferred_nat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_type_loc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_description_loc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_transferred_loc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "month",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dow_avg_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_rolling_3_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_rolling_7_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_avg_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_rolling_3_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_rolling_7_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_7_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_14_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_7_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_14_transactions",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7c00bd2a-c15e-4135-95f6-959b2b41e679",
       "rows": [
        [
         "0",
         "1",
         "0",
         "7.0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "53.9",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2017",
         "2",
         "1",
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "1",
         "1",
         "0.0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "53.9",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2017",
         "2",
         "1",
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "1",
         "2",
         "7.0",
         "1",
         "0",
         "0",
         "0",
         "13",
         "53.9",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2017",
         "2",
         "1",
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "1",
         "3",
         "2399.0",
         "28",
         "0",
         "0",
         "0",
         "13",
         "53.9",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2017",
         "2",
         "1",
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "1",
         "4",
         "1.0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "53.9",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2017",
         "2",
         "1",
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 29,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>store_type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>oil</th>\n",
       "      <th>h_type_nat</th>\n",
       "      <th>...</th>\n",
       "      <th>dow_avg_sales</th>\n",
       "      <th>dow_rolling_3_sales</th>\n",
       "      <th>dow_rolling_7_sales</th>\n",
       "      <th>dow_avg_transactions</th>\n",
       "      <th>dow_rolling_3_transactions</th>\n",
       "      <th>dow_rolling_7_transactions</th>\n",
       "      <th>rolling_7_sales</th>\n",
       "      <th>rolling_14_sales</th>\n",
       "      <th>rolling_7_transactions</th>\n",
       "      <th>rolling_14_transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>53.9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>53.9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>53.9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>53.9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>53.9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr  family   sales  onpromotion  city  state  store_type  cluster  \\\n",
       "0          1       0     7.0            0     0      0           0       13   \n",
       "1          1       1     0.0            0     0      0           0       13   \n",
       "2          1       2     7.0            1     0      0           0       13   \n",
       "3          1       3  2399.0           28     0      0           0       13   \n",
       "4          1       4     1.0            0     0      0           0       13   \n",
       "\n",
       "    oil  h_type_nat  ...  dow_avg_sales  dow_rolling_3_sales  \\\n",
       "0  53.9           0  ...            0.0                  0.0   \n",
       "1  53.9           0  ...            0.0                  0.0   \n",
       "2  53.9           0  ...            0.0                  0.0   \n",
       "3  53.9           0  ...            0.0                  0.0   \n",
       "4  53.9           0  ...            0.0                  0.0   \n",
       "\n",
       "   dow_rolling_7_sales  dow_avg_transactions  dow_rolling_3_transactions  \\\n",
       "0                  0.0                   0.0                         0.0   \n",
       "1                  0.0                   0.0                         0.0   \n",
       "2                  0.0                   0.0                         0.0   \n",
       "3                  0.0                   0.0                         0.0   \n",
       "4                  0.0                   0.0                         0.0   \n",
       "\n",
       "   dow_rolling_7_transactions  rolling_7_sales  rolling_14_sales  \\\n",
       "0                         0.0              0.0               0.0   \n",
       "1                         0.0              0.0               0.0   \n",
       "2                         0.0              0.0               0.0   \n",
       "3                         0.0              0.0               0.0   \n",
       "4                         0.0              0.0               0.0   \n",
       "\n",
       "   rolling_7_transactions  rolling_14_transactions  \n",
       "0                     0.0                      0.0  \n",
       "1                     0.0                      0.0  \n",
       "2                     0.0                      0.0  \n",
       "3                     0.0                      0.0  \n",
       "4                     0.0                      0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = 'data/'\n",
    "df = pd.read_csv(os.path.join(data_dir, 'train_data.csv'))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['store_nbr', 'city', 'state', 'store_type', 'cluster', 'h_type_nat', 'h_description_nat', 'h_transferred_nat', 'h_type_loc', 'h_description_loc', 'h_transferred_loc', 'month', 'day', 'day_of_week']\n",
      "Index(['store_nbr', 'family', 'sales', 'onpromotion', 'city', 'state',\n",
      "       'store_type', 'cluster', 'oil', 'h_type_nat', 'h_description_nat',\n",
      "       'h_transferred_nat', 'h_type_loc', 'h_description_loc',\n",
      "       'h_transferred_loc', 'year', 'month', 'day', 'day_of_week',\n",
      "       'dow_avg_sales', 'dow_rolling_3_sales', 'dow_rolling_7_sales',\n",
      "       'dow_avg_transactions', 'dow_rolling_3_transactions',\n",
      "       'dow_rolling_7_transactions', 'rolling_7_sales', 'rolling_14_sales',\n",
      "       'rolling_7_transactions', 'rolling_14_transactions'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "min_max_cols = ['store_nbr', 'city', 'state', 'store_type', 'cluster', 'h_type_nat', 'h_description_nat', 'h_transferred_nat', 'h_type_loc', 'h_description_loc', 'h_transferred_loc', 'month', 'day', 'day_of_week']\n",
    "normalize_cols = ['onpromotion', 'oil', 'dow_avg_sales', 'dow_rolling_3_sales', 'rolling_7_sales', 'rolling_14_sales', 'dow_avg_transactions', 'dow_rolling_3_transactions', 'rolling_7_transactions', 'rolling_14_transactions']\n",
    "x_cols = min_max_cols + normalize_cols\n",
    "y_cols = ['sales']\n",
    "split_col = 'family'\n",
    "\n",
    "print(min_max_cols)\n",
    "print(df.columns)\n",
    "\n",
    "final_run = False\n",
    "\n",
    "if final_run:\n",
    "    train_df = df\n",
    "else:\n",
    "    rows_after = (df['year'] >= 2017) & (df['month'] >= 8) & (df['day'] >= 1)\n",
    "    rows_before = ~rows_after\n",
    "\n",
    "    train_df = df[rows_before]\n",
    "    val_df = df[rows_after]\n",
    "\n",
    "\n",
    "train_df_by_cluster = {}\n",
    "scaler_x_by_cluster = {}\n",
    "scaler_y_by_cluster = {}\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    cluster_df, cluster_min_max_scaler, cluster_normalize_scaler, cluster_y_scaler = create_scaled_data_by_col(train_df, min_max_cols, normalize_cols, y_cols, split_col, cluster)\n",
    "    train_df_by_cluster[cluster] = cluster_df\n",
    "    scaler_x_by_cluster[cluster] = (cluster_min_max_scaler, cluster_normalize_scaler)\n",
    "    scaler_y_by_cluster[cluster] = cluster_y_scaler\n",
    "\n",
    "if not final_run:\n",
    "    val_df_by_cluster = {}\n",
    "\n",
    "    for cluster in df[split_col].unique():\n",
    "        val_cluster_min_max_scaler, val_cluster_normalize_scaler = scaler_x_by_cluster[cluster]\n",
    "        val_cluster_y_scaler = scaler_y_by_cluster[cluster]\n",
    "\n",
    "        val_cluster_df = val_df[val_df[split_col] == cluster]\n",
    "        val_cluster_df = val_cluster_df.drop(columns=split_col)\n",
    "\n",
    "        val_cluster_x_min_max = val_cluster_df[min_max_cols].values.astype(np.float32)\n",
    "        val_cluster_x_normalize = val_cluster_df[normalize_cols].values.astype(np.float32)\n",
    "        val_cluster_y = val_cluster_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "        val_cluster_x_min_max = val_cluster_min_max_scaler.transform(val_cluster_x_min_max)\n",
    "        val_cluster_x_normalize = val_cluster_normalize_scaler.transform(val_cluster_x_normalize)\n",
    "        val_cluster_y = val_cluster_y_scaler.transform(val_cluster_y)\n",
    "\n",
    "        val_cluster_df[min_max_cols] = val_cluster_x_min_max\n",
    "        val_cluster_df[normalize_cols] = val_cluster_x_normalize\n",
    "        val_cluster_df[y_cols] = val_cluster_y\n",
    "\n",
    "        val_df_by_cluster[cluster] = val_cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim=64, num_hidden_layers=2):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        self.head = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_hidden_layers):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim ))\n",
    "\n",
    "        self.stem = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.head(x))\n",
    "\n",
    "        for layer in self.stem:\n",
    "            x =  x + self.act(layer(x))\n",
    "\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_by_cluster = {}\n",
    "train_params = {\n",
    "                \"criterion\": nn.L1Loss,\n",
    "                \"optimizer\": torch.optim.AdamW,\n",
    "                \"optimizer__weight_decay\": 1e-8,\n",
    "                #'train_split' : None,\n",
    "                #\"train_split\": predefined_split(Dataset(val_x, val_y)),\n",
    "                \"lr\": 0.001,\n",
    "                \"batch_size\": 128,\n",
    "                \"max_epochs\": 1000,\n",
    "                \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                \"iterator_train__shuffle\": False,\n",
    "                \"iterator_train__num_workers\": 2,\n",
    "                \"iterator_train__pin_memory\": True,\n",
    "                \"iterator_valid__shuffle\": False,\n",
    "                \"iterator_valid__num_workers\": 2,\n",
    "                \"iterator_valid__pin_memory\": True,\n",
    "                \"verbose\": 2,\n",
    "        }\n",
    "\n",
    "net_params = {\n",
    "    'input_dim': len(x_cols),\n",
    "    'out_dim': 1,\n",
    "    'hidden_dim': 200,\n",
    "    'num_hidden_layers': 6,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.5912\u001b[0m        \u001b[32m0.5464\u001b[0m     +  0.3463\n",
      "      2        \u001b[36m0.5566\u001b[0m        \u001b[32m0.4985\u001b[0m     +  0.3242\n",
      "      3        \u001b[36m0.5363\u001b[0m        \u001b[32m0.4945\u001b[0m     +  0.3751\n",
      "      4        \u001b[36m0.5159\u001b[0m        \u001b[32m0.4933\u001b[0m     +  0.3629\n",
      "      5        \u001b[36m0.5120\u001b[0m        \u001b[32m0.4913\u001b[0m     +  0.3543\n",
      "      6        \u001b[36m0.5070\u001b[0m        \u001b[32m0.4904\u001b[0m     +  0.3464\n",
      "      7        \u001b[36m0.5051\u001b[0m        0.4915        0.3461\n",
      "      8        \u001b[36m0.5011\u001b[0m        \u001b[32m0.4900\u001b[0m     +  0.3458\n",
      "      9        \u001b[36m0.4967\u001b[0m        \u001b[32m0.4890\u001b[0m     +  0.3372\n",
      "     10        \u001b[36m0.4915\u001b[0m        0.4894        0.3510\n",
      "     11        \u001b[36m0.4899\u001b[0m        0.4901        0.3383\n",
      "     12        \u001b[36m0.4880\u001b[0m        0.4913        0.3431\n",
      "     13        \u001b[36m0.4856\u001b[0m        0.4920        0.3521\n",
      "     14        \u001b[36m0.4820\u001b[0m        0.4910        0.3563\n",
      "     15        \u001b[36m0.4812\u001b[0m        0.4932        0.3492\n",
      "     16        0.4935        0.4920        0.3380\n",
      "     17        0.4854        \u001b[32m0.4880\u001b[0m     +  0.3268\n",
      "     18        \u001b[36m0.4797\u001b[0m        0.4922        0.3219\n",
      "     19        \u001b[36m0.4736\u001b[0m        0.4894        0.3416\n",
      "     20        \u001b[36m0.4697\u001b[0m        0.4906        0.3634\n",
      "     21        \u001b[36m0.4666\u001b[0m        0.4898        0.3498\n",
      "     22        \u001b[36m0.4661\u001b[0m        0.4905        0.3508\n",
      "     23        0.4662        \u001b[32m0.4866\u001b[0m     +  0.3373\n",
      "     24        \u001b[36m0.4653\u001b[0m        0.4914        0.3447\n",
      "     25        0.4681        0.4879        0.3379\n",
      "     26        \u001b[36m0.4633\u001b[0m        0.4919        0.3580\n",
      "     27        \u001b[36m0.4577\u001b[0m        0.4919        0.3183\n",
      "     28        \u001b[36m0.4559\u001b[0m        0.4949        0.3464\n",
      "     29        \u001b[36m0.4537\u001b[0m        0.5074        0.3540\n",
      "     30        0.4546        0.4957        0.3481\n",
      "     31        \u001b[36m0.4478\u001b[0m        0.4972        0.3553\n",
      "     32        \u001b[36m0.4434\u001b[0m        0.4954        0.3516\n",
      "     33        \u001b[36m0.4421\u001b[0m        0.5007        0.3454\n",
      "     34        \u001b[36m0.4393\u001b[0m        0.5011        0.3303\n",
      "     35        \u001b[36m0.4365\u001b[0m        0.5024        0.3569\n",
      "     36        \u001b[36m0.4342\u001b[0m        0.5041        0.3376\n",
      "     37        0.4363        0.5021        0.3357\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4134\u001b[0m        \u001b[32m0.2961\u001b[0m     +  0.3388\n",
      "      2        \u001b[36m0.3396\u001b[0m        \u001b[32m0.2766\u001b[0m     +  0.3694\n",
      "      3        0.3397        0.2933        0.3469\n",
      "      4        \u001b[36m0.3326\u001b[0m        \u001b[32m0.2654\u001b[0m     +  0.3471\n",
      "      5        0.3476        0.2785        0.3556\n",
      "      6        \u001b[36m0.3306\u001b[0m        \u001b[32m0.2613\u001b[0m     +  0.3443\n",
      "      7        \u001b[36m0.3159\u001b[0m        0.2613        0.3545\n",
      "      8        0.3199        0.2813        0.3557\n",
      "      9        0.3175        0.2753        0.3347\n",
      "     10        0.3212        0.2719        0.3574\n",
      "     11        \u001b[36m0.3141\u001b[0m        0.2807        0.3468\n",
      "     12        0.3207        0.2677        0.3302\n",
      "     13        \u001b[36m0.3057\u001b[0m        0.2656        0.3683\n",
      "     14        \u001b[36m0.3034\u001b[0m        0.2626        0.3508\n",
      "     15        \u001b[36m0.3011\u001b[0m        0.2640        0.3547\n",
      "     16        0.3025        \u001b[32m0.2593\u001b[0m     +  0.3469\n",
      "     17        0.3022        0.2604        0.3601\n",
      "     18        0.3016        0.2691        0.3449\n",
      "     19        \u001b[36m0.3002\u001b[0m        0.2701        0.3353\n",
      "     20        \u001b[36m0.2980\u001b[0m        0.2707        0.3509\n",
      "     21        0.2990        0.2669        0.3390\n",
      "     22        0.3042        0.2670        0.3433\n",
      "     23        0.2985        0.2599        0.3388\n",
      "     24        \u001b[36m0.2967\u001b[0m        0.2623        0.3465\n",
      "     25        \u001b[36m0.2963\u001b[0m        0.2612        0.3538\n",
      "     26        0.2966        0.2633        0.3404\n",
      "     27        \u001b[36m0.2945\u001b[0m        0.2633        0.3341\n",
      "     28        0.2961        0.2663        0.3347\n",
      "     29        0.2957        0.2610        0.3445\n",
      "     30        0.2994        0.2616        0.3552\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4879\u001b[0m        \u001b[32m0.4781\u001b[0m     +  0.3447\n",
      "      2        \u001b[36m0.4552\u001b[0m        0.4835        0.3415\n",
      "      3        \u001b[36m0.4358\u001b[0m        \u001b[32m0.4757\u001b[0m     +  0.3440\n",
      "      4        \u001b[36m0.3959\u001b[0m        \u001b[32m0.4645\u001b[0m     +  0.3475\n",
      "      5        0.4004        \u001b[32m0.4525\u001b[0m     +  0.3250\n",
      "      6        \u001b[36m0.3884\u001b[0m        \u001b[32m0.4427\u001b[0m     +  0.3436\n",
      "      7        0.3892        \u001b[32m0.4378\u001b[0m     +  0.3502\n",
      "      8        0.3920        0.4393        0.3649\n",
      "      9        0.3910        0.4399        0.3429\n",
      "     10        \u001b[36m0.3870\u001b[0m        \u001b[32m0.4345\u001b[0m     +  0.3445\n",
      "     11        \u001b[36m0.3839\u001b[0m        \u001b[32m0.4315\u001b[0m     +  0.3396\n",
      "     12        \u001b[36m0.3805\u001b[0m        0.4355        0.3470\n",
      "     13        \u001b[36m0.3711\u001b[0m        0.4409        0.3491\n",
      "     14        \u001b[36m0.3662\u001b[0m        0.4419        0.3454\n",
      "     15        \u001b[36m0.3633\u001b[0m        0.4441        0.3410\n",
      "     16        \u001b[36m0.3591\u001b[0m        0.4398        0.3298\n",
      "     17        \u001b[36m0.3585\u001b[0m        0.4371        0.3431\n",
      "     18        \u001b[36m0.3582\u001b[0m        0.4481        0.3373\n",
      "     19        \u001b[36m0.3558\u001b[0m        0.4352        0.3308\n",
      "     20        \u001b[36m0.3516\u001b[0m        0.4324        0.3597\n",
      "     21        \u001b[36m0.3476\u001b[0m        0.4375        0.3429\n",
      "     22        0.3533        0.4355        0.3499\n",
      "     23        \u001b[36m0.3462\u001b[0m        0.4357        0.3493\n",
      "     24        \u001b[36m0.3427\u001b[0m        \u001b[32m0.4303\u001b[0m     +  0.3452\n",
      "     25        \u001b[36m0.3375\u001b[0m        \u001b[32m0.4292\u001b[0m     +  0.3583\n",
      "     26        \u001b[36m0.3329\u001b[0m        0.4302        0.3368\n",
      "     27        \u001b[36m0.3306\u001b[0m        0.4321        0.3578\n",
      "     28        \u001b[36m0.3281\u001b[0m        0.4316        0.3637\n",
      "     29        \u001b[36m0.3260\u001b[0m        0.4310        0.3530\n",
      "     30        \u001b[36m0.3236\u001b[0m        0.4328        0.3771\n",
      "     31        \u001b[36m0.3225\u001b[0m        0.4325        0.3785\n",
      "     32        \u001b[36m0.3201\u001b[0m        0.4381        0.3525\n",
      "     33        \u001b[36m0.3172\u001b[0m        0.4354        0.3429\n",
      "     34        \u001b[36m0.3155\u001b[0m        0.4345        0.3366\n",
      "     35        \u001b[36m0.3138\u001b[0m        0.4363        0.3534\n",
      "     36        \u001b[36m0.3124\u001b[0m        0.4369        0.3404\n",
      "     37        \u001b[36m0.3112\u001b[0m        0.4344        0.3420\n",
      "     38        \u001b[36m0.3094\u001b[0m        0.4329        0.3469\n",
      "     39        \u001b[36m0.3085\u001b[0m        0.4307        0.3368\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3684\u001b[0m        \u001b[32m0.2495\u001b[0m     +  0.3316\n",
      "      2        \u001b[36m0.3077\u001b[0m        \u001b[32m0.2050\u001b[0m     +  0.3419\n",
      "      3        \u001b[36m0.2961\u001b[0m        0.2126        0.3354\n",
      "      4        \u001b[36m0.2525\u001b[0m        0.2113        0.3295\n",
      "      5        \u001b[36m0.2430\u001b[0m        0.2242        0.3542\n",
      "      6        \u001b[36m0.2414\u001b[0m        0.2422        0.3507\n",
      "      7        \u001b[36m0.2361\u001b[0m        0.2179        0.3484\n",
      "      8        \u001b[36m0.2293\u001b[0m        0.2103        0.3521\n",
      "      9        0.2406        0.2491        0.3492\n",
      "     10        0.2353        0.2492        0.3623\n",
      "     11        \u001b[36m0.2260\u001b[0m        0.2578        0.3523\n",
      "     12        \u001b[36m0.2123\u001b[0m        0.2446        0.3537\n",
      "     13        0.2166        0.2434        0.3274\n",
      "     14        \u001b[36m0.2086\u001b[0m        0.2395        0.3393\n",
      "     15        0.2152        0.2458        0.3417\n",
      "     16        \u001b[36m0.2026\u001b[0m        0.2289        0.3437\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4014\u001b[0m        \u001b[32m0.1117\u001b[0m     +  0.3693\n",
      "      2        \u001b[36m0.3419\u001b[0m        \u001b[32m0.0400\u001b[0m     +  0.3630\n",
      "      3        \u001b[36m0.3325\u001b[0m        0.0447        0.3623\n",
      "      4        \u001b[36m0.3211\u001b[0m        \u001b[32m0.0349\u001b[0m     +  0.3384\n",
      "      5        \u001b[36m0.2985\u001b[0m        0.0362        0.3391\n",
      "      6        0.3026        0.0429        0.3560\n",
      "      7        \u001b[36m0.2892\u001b[0m        \u001b[32m0.0326\u001b[0m     +  0.3425\n",
      "      8        0.2962        0.0350        0.3371\n",
      "      9        0.2950        0.0356        0.3294\n",
      "     10        0.2917        \u001b[32m0.0278\u001b[0m     +  0.3506\n",
      "     11        \u001b[36m0.2798\u001b[0m        \u001b[32m0.0254\u001b[0m     +  0.3393\n",
      "     12        0.2828        0.0256        0.3298\n",
      "     13        0.2826        \u001b[32m0.0237\u001b[0m     +  0.3459\n",
      "     14        \u001b[36m0.2796\u001b[0m        0.0253        0.3384\n",
      "     15        \u001b[36m0.2763\u001b[0m        0.0276        0.3525\n",
      "     16        \u001b[36m0.2741\u001b[0m        0.0316        0.3263\n",
      "     17        0.2789        0.0258        0.3342\n",
      "     18        0.2861        0.0268        0.3393\n",
      "     19        0.2915        0.0269        0.3434\n",
      "     20        0.2795        \u001b[32m0.0233\u001b[0m     +  0.3404\n",
      "     21        \u001b[36m0.2721\u001b[0m        0.0259        0.3419\n",
      "     22        0.2724        \u001b[32m0.0221\u001b[0m     +  0.3366\n",
      "     23        \u001b[36m0.2687\u001b[0m        \u001b[32m0.0212\u001b[0m     +  0.3475\n",
      "     24        \u001b[36m0.2672\u001b[0m        0.0212        0.3605\n",
      "     25        \u001b[36m0.2668\u001b[0m        \u001b[32m0.0204\u001b[0m     +  0.3460\n",
      "     26        0.2686        0.0208        0.3520\n",
      "     27        0.2696        0.0284        0.3573\n",
      "     28        0.2700        0.0208        0.3275\n",
      "     29        0.2677        0.0236        0.3526\n",
      "     30        0.2672        0.0212        0.3469\n",
      "     31        \u001b[36m0.2666\u001b[0m        0.0216        0.3619\n",
      "     32        0.2686        0.0205        0.3393\n",
      "     33        \u001b[36m0.2661\u001b[0m        0.0228        0.3399\n",
      "     34        \u001b[36m0.2656\u001b[0m        0.0265        0.3462\n",
      "     35        \u001b[36m0.2613\u001b[0m        0.0205        0.3337\n",
      "     36        0.2620        0.0206        0.3564\n",
      "     37        \u001b[36m0.2611\u001b[0m        0.0245        0.3526\n",
      "     38        0.2615        0.0223        0.3445\n",
      "     39        \u001b[36m0.2579\u001b[0m        0.0218        0.3430\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3564\u001b[0m        \u001b[32m0.1947\u001b[0m     +  0.3307\n",
      "      2        \u001b[36m0.2979\u001b[0m        \u001b[32m0.1824\u001b[0m     +  0.3415\n",
      "      3        \u001b[36m0.2833\u001b[0m        0.2178        0.3387\n",
      "      4        0.2927        \u001b[32m0.1807\u001b[0m     +  0.3412\n",
      "      5        \u001b[36m0.2327\u001b[0m        \u001b[32m0.1805\u001b[0m     +  0.3325\n",
      "      6        0.2493        \u001b[32m0.1766\u001b[0m     +  0.3448\n",
      "      7        0.2398        0.1782        0.3436\n",
      "      8        0.2480        0.1856        0.3400\n",
      "      9        \u001b[36m0.2304\u001b[0m        0.1849        0.3399\n",
      "     10        0.2427        0.1914        0.3382\n",
      "     11        0.2336        0.1840        0.3445\n",
      "     12        \u001b[36m0.2264\u001b[0m        0.1802        0.3420\n",
      "     13        \u001b[36m0.2256\u001b[0m        \u001b[32m0.1676\u001b[0m     +  0.3607\n",
      "     14        0.2275        0.1746        0.3665\n",
      "     15        \u001b[36m0.2101\u001b[0m        0.1699        0.3440\n",
      "     16        \u001b[36m0.2093\u001b[0m        0.1744        0.3711\n",
      "     17        \u001b[36m0.2018\u001b[0m        0.1689        0.3523\n",
      "     18        \u001b[36m0.1962\u001b[0m        0.1682        0.3461\n",
      "     19        \u001b[36m0.1941\u001b[0m        \u001b[32m0.1671\u001b[0m     +  0.3208\n",
      "     20        \u001b[36m0.1881\u001b[0m        0.1916        0.3285\n",
      "     21        \u001b[36m0.1838\u001b[0m        0.1844        0.3416\n",
      "     22        \u001b[36m0.1807\u001b[0m        0.1819        0.3542\n",
      "     23        \u001b[36m0.1791\u001b[0m        0.1800        0.3448\n",
      "     24        0.1867        0.1810        0.3462\n",
      "     25        0.1828        0.1812        0.3266\n",
      "     26        0.1816        0.1857        0.3429\n",
      "     27        \u001b[36m0.1756\u001b[0m        0.1832        0.3398\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.5093\u001b[0m        \u001b[32m0.3822\u001b[0m     +  0.3625\n",
      "      2        \u001b[36m0.4315\u001b[0m        0.3865        0.3528\n",
      "      3        \u001b[36m0.4189\u001b[0m        \u001b[32m0.3537\u001b[0m     +  0.3317\n",
      "      4        0.4205        0.3599        0.3428\n",
      "      5        0.4208        0.3654        0.3417\n",
      "      6        \u001b[36m0.4063\u001b[0m        0.3607        0.3472\n",
      "      7        \u001b[36m0.3977\u001b[0m        0.3565        0.3386\n",
      "      8        \u001b[36m0.3931\u001b[0m        0.3560        0.3648\n",
      "      9        \u001b[36m0.3907\u001b[0m        0.3558        0.3571\n",
      "     10        \u001b[36m0.3876\u001b[0m        0.3594        0.3384\n",
      "     11        \u001b[36m0.3832\u001b[0m        0.3574        0.3597\n",
      "     12        \u001b[36m0.3802\u001b[0m        0.3566        0.3368\n",
      "     13        \u001b[36m0.3782\u001b[0m        0.3559        0.3416\n",
      "     14        \u001b[36m0.3753\u001b[0m        0.3572        0.3581\n",
      "     15        \u001b[36m0.3735\u001b[0m        0.3566        0.3501\n",
      "     16        \u001b[36m0.3716\u001b[0m        0.3583        0.3327\n",
      "     17        \u001b[36m0.3700\u001b[0m        0.3590        0.3386\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4407\u001b[0m        \u001b[32m0.3681\u001b[0m     +  0.3408\n",
      "      2        \u001b[36m0.4006\u001b[0m        \u001b[32m0.3327\u001b[0m     +  0.3572\n",
      "      3        \u001b[36m0.3977\u001b[0m        0.3577        0.3485\n",
      "      4        \u001b[36m0.3527\u001b[0m        0.4061        0.3514\n",
      "      5        \u001b[36m0.3425\u001b[0m        0.3398        0.3521\n",
      "      6        0.3446        \u001b[32m0.3148\u001b[0m     +  0.3254\n",
      "      7        \u001b[36m0.3420\u001b[0m        0.3379        0.3519\n",
      "      8        \u001b[36m0.3322\u001b[0m        0.3764        0.3568\n",
      "      9        \u001b[36m0.3145\u001b[0m        0.3572        0.3627\n",
      "     10        \u001b[36m0.3134\u001b[0m        0.3534        0.3445\n",
      "     11        \u001b[36m0.3064\u001b[0m        0.3727        0.3426\n",
      "     12        0.3097        0.3647        0.3456\n",
      "     13        \u001b[36m0.3022\u001b[0m        0.3573        0.3562\n",
      "     14        0.3032        0.3725        0.3451\n",
      "     15        \u001b[36m0.3004\u001b[0m        0.3730        0.3473\n",
      "     16        \u001b[36m0.2878\u001b[0m        0.3697        0.3379\n",
      "     17        \u001b[36m0.2842\u001b[0m        0.3753        0.3367\n",
      "     18        \u001b[36m0.2792\u001b[0m        0.3703        0.3611\n",
      "     19        0.2920        0.4169        0.3487\n",
      "     20        \u001b[36m0.2707\u001b[0m        0.4016        0.3351\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3802\u001b[0m        \u001b[32m0.2868\u001b[0m     +  0.3532\n",
      "      2        \u001b[36m0.3199\u001b[0m        \u001b[32m0.1508\u001b[0m     +  0.3418\n",
      "      3        \u001b[36m0.2916\u001b[0m        0.1547        0.3155\n",
      "      4        \u001b[36m0.2313\u001b[0m        \u001b[32m0.1507\u001b[0m     +  0.3436\n",
      "      5        \u001b[36m0.2278\u001b[0m        0.1568        0.3505\n",
      "      6        \u001b[36m0.2153\u001b[0m        0.1508        0.3550\n",
      "      7        0.2193        \u001b[32m0.1468\u001b[0m     +  0.3482\n",
      "      8        0.2166        \u001b[32m0.1459\u001b[0m     +  0.3507\n",
      "      9        0.2167        0.1480        0.3557\n",
      "     10        0.2201        0.1480        0.3489\n",
      "     11        \u001b[36m0.2083\u001b[0m        \u001b[32m0.1401\u001b[0m     +  0.3527\n",
      "     12        0.2108        \u001b[32m0.1378\u001b[0m     +  0.3332\n",
      "     13        \u001b[36m0.2045\u001b[0m        0.1386        0.3263\n",
      "     14        \u001b[36m0.2025\u001b[0m        0.1418        0.3456\n",
      "     15        \u001b[36m0.1996\u001b[0m        0.1460        0.3563\n",
      "     16        \u001b[36m0.1973\u001b[0m        0.1480        0.3631\n",
      "     17        \u001b[36m0.1906\u001b[0m        0.1402        0.3485\n",
      "     18        \u001b[36m0.1823\u001b[0m        0.1397        0.3160\n",
      "     19        0.1914        0.1466        0.3209\n",
      "     20        0.1913        0.1466        0.3458\n",
      "     21        0.1948        0.1457        0.3639\n",
      "     22        \u001b[36m0.1810\u001b[0m        0.1453        0.3655\n",
      "     23        0.1946        0.1450        0.3500\n",
      "     24        \u001b[36m0.1773\u001b[0m        0.1475        0.3412\n",
      "     25        0.1792        0.1502        0.3537\n",
      "     26        \u001b[36m0.1620\u001b[0m        0.1499        0.3606\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4019\u001b[0m        \u001b[32m0.3183\u001b[0m     +  0.3449\n",
      "      2        \u001b[36m0.3522\u001b[0m        \u001b[32m0.2200\u001b[0m     +  0.3480\n",
      "      3        \u001b[36m0.3064\u001b[0m        0.2258        0.3375\n",
      "      4        \u001b[36m0.2695\u001b[0m        \u001b[32m0.2128\u001b[0m     +  0.3364\n",
      "      5        \u001b[36m0.2658\u001b[0m        0.2183        0.3510\n",
      "      6        \u001b[36m0.2574\u001b[0m        0.2162        0.3442\n",
      "      7        \u001b[36m0.2523\u001b[0m        0.2261        0.3394\n",
      "      8        \u001b[36m0.2453\u001b[0m        0.2192        0.3508\n",
      "      9        \u001b[36m0.2388\u001b[0m        0.2201        0.3561\n",
      "     10        \u001b[36m0.2303\u001b[0m        \u001b[32m0.2126\u001b[0m     +  0.3489\n",
      "     11        \u001b[36m0.2294\u001b[0m        0.2132        0.3542\n",
      "     12        0.2350        0.2251        0.3355\n",
      "     13        0.2418        0.2366        0.3540\n",
      "     14        \u001b[36m0.2155\u001b[0m        0.2194        0.3601\n",
      "     15        \u001b[36m0.2131\u001b[0m        0.2211        0.3564\n",
      "     16        \u001b[36m0.2100\u001b[0m        0.2228        0.3389\n",
      "     17        \u001b[36m0.2080\u001b[0m        0.2559        0.3420\n",
      "     18        \u001b[36m0.2045\u001b[0m        0.2599        0.3347\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3840\u001b[0m        \u001b[32m0.2169\u001b[0m     +  0.3342\n",
      "      2        \u001b[36m0.2801\u001b[0m        \u001b[32m0.2155\u001b[0m     +  0.3299\n",
      "      3        \u001b[36m0.2670\u001b[0m        0.2296        0.3484\n",
      "      4        0.2706        0.2227        0.3590\n",
      "      5        0.2687        0.2490        0.3435\n",
      "      6        0.2708        0.2427        0.3519\n",
      "      7        \u001b[36m0.2592\u001b[0m        0.2317        0.3506\n",
      "      8        \u001b[36m0.2506\u001b[0m        0.2590        0.3294\n",
      "      9        0.2543        \u001b[32m0.2063\u001b[0m     +  0.3401\n",
      "     10        \u001b[36m0.2493\u001b[0m        \u001b[32m0.2058\u001b[0m     +  0.3396\n",
      "     11        \u001b[36m0.2409\u001b[0m        0.2072        0.3436\n",
      "     12        \u001b[36m0.2395\u001b[0m        0.2067        0.3607\n",
      "     13        \u001b[36m0.2391\u001b[0m        0.2066        0.3339\n",
      "     14        \u001b[36m0.2367\u001b[0m        0.2080        0.3472\n",
      "     15        \u001b[36m0.2342\u001b[0m        0.2099        0.3476\n",
      "     16        \u001b[36m0.2316\u001b[0m        0.2152        0.3256\n",
      "     17        0.2338        0.2135        0.3614\n",
      "     18        \u001b[36m0.2314\u001b[0m        0.2128        0.3562\n",
      "     19        \u001b[36m0.2294\u001b[0m        0.2137        0.3449\n",
      "     20        \u001b[36m0.2282\u001b[0m        0.2135        0.3480\n",
      "     21        \u001b[36m0.2257\u001b[0m        0.2132        0.3436\n",
      "     22        \u001b[36m0.2244\u001b[0m        0.2291        0.3268\n",
      "     23        \u001b[36m0.2226\u001b[0m        0.2275        0.3547\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3695\u001b[0m        \u001b[32m0.2482\u001b[0m     +  0.3617\n",
      "      2        \u001b[36m0.3117\u001b[0m        \u001b[32m0.2397\u001b[0m     +  0.3447\n",
      "      3        0.3514        \u001b[32m0.2143\u001b[0m     +  0.3495\n",
      "      4        0.3120        \u001b[32m0.2061\u001b[0m     +  0.3241\n",
      "      5        \u001b[36m0.2769\u001b[0m        \u001b[32m0.2056\u001b[0m     +  0.3397\n",
      "      6        \u001b[36m0.2753\u001b[0m        \u001b[32m0.2044\u001b[0m     +  0.3358\n",
      "      7        \u001b[36m0.2671\u001b[0m        0.2055        0.3505\n",
      "      8        \u001b[36m0.2653\u001b[0m        0.2055        0.3561\n",
      "      9        \u001b[36m0.2576\u001b[0m        \u001b[32m0.2032\u001b[0m     +  0.3396\n",
      "     10        \u001b[36m0.2535\u001b[0m        \u001b[32m0.2024\u001b[0m     +  0.3420\n",
      "     11        \u001b[36m0.2446\u001b[0m        \u001b[32m0.2009\u001b[0m     +  0.3467\n",
      "     12        0.2482        \u001b[32m0.2003\u001b[0m     +  0.3497\n",
      "     13        \u001b[36m0.2442\u001b[0m        0.2008        0.3438\n",
      "     14        \u001b[36m0.2433\u001b[0m        0.2013        0.3366\n",
      "     15        0.2468        0.2065        0.3700\n",
      "     16        \u001b[36m0.2360\u001b[0m        0.2124        0.3186\n",
      "     17        \u001b[36m0.2336\u001b[0m        0.2037        0.3431\n",
      "     18        0.2428        0.2069        0.3344\n",
      "     19        0.2382        0.2040        0.3213\n",
      "     20        \u001b[36m0.2302\u001b[0m        0.2029        0.3227\n",
      "     21        \u001b[36m0.2266\u001b[0m        0.2075        0.3233\n",
      "     22        \u001b[36m0.2231\u001b[0m        0.2021        0.3262\n",
      "     23        0.2278        0.2062        0.3384\n",
      "     24        0.2276        0.2083        0.3427\n",
      "     25        \u001b[36m0.2180\u001b[0m        0.2079        0.3715\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4292\u001b[0m        \u001b[32m0.3194\u001b[0m     +  0.3497\n",
      "      2        \u001b[36m0.3556\u001b[0m        \u001b[32m0.2634\u001b[0m     +  0.3320\n",
      "      3        \u001b[36m0.3069\u001b[0m        \u001b[32m0.2498\u001b[0m     +  0.3595\n",
      "      4        \u001b[36m0.2889\u001b[0m        0.2569        0.3434\n",
      "      5        \u001b[36m0.2772\u001b[0m        \u001b[32m0.2463\u001b[0m     +  0.3332\n",
      "      6        \u001b[36m0.2610\u001b[0m        \u001b[32m0.2317\u001b[0m     +  0.3421\n",
      "      7        \u001b[36m0.2529\u001b[0m        \u001b[32m0.2243\u001b[0m     +  0.3462\n",
      "      8        0.2545        0.2295        0.3480\n",
      "      9        \u001b[36m0.2377\u001b[0m        \u001b[32m0.2222\u001b[0m     +  0.3543\n",
      "     10        0.2501        0.2297        0.3347\n",
      "     11        \u001b[36m0.2373\u001b[0m        0.2322        0.3462\n",
      "     12        0.2411        0.2301        0.3544\n",
      "     13        \u001b[36m0.2233\u001b[0m        0.2272        0.3519\n",
      "     14        0.2415        0.2336        0.3843\n",
      "     15        \u001b[36m0.2164\u001b[0m        \u001b[32m0.2217\u001b[0m     +  0.3391\n",
      "     16        0.2394        0.2473        0.3462\n",
      "     17        0.2177        0.2515        0.3400\n",
      "     18        0.2168        0.2512        0.3626\n",
      "     19        \u001b[36m0.2055\u001b[0m        0.2554        0.3510\n",
      "     20        0.2297        0.2575        0.3616\n",
      "     21        0.2070        0.2510        0.3691\n",
      "     22        0.2186        0.2906        0.3494\n",
      "     23        \u001b[36m0.1958\u001b[0m        0.2920        0.3428\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4115\u001b[0m        \u001b[32m0.3773\u001b[0m     +  0.3503\n",
      "      2        \u001b[36m0.3484\u001b[0m        0.4180        0.3432\n",
      "      3        0.3523        0.3881        0.3356\n",
      "      4        \u001b[36m0.3323\u001b[0m        \u001b[32m0.3754\u001b[0m     +  0.3636\n",
      "      5        \u001b[36m0.3276\u001b[0m        0.3755        0.3529\n",
      "      6        \u001b[36m0.3162\u001b[0m        0.3837        0.3485\n",
      "      7        \u001b[36m0.3129\u001b[0m        \u001b[32m0.3659\u001b[0m     +  0.3324\n",
      "      8        \u001b[36m0.3095\u001b[0m        0.3706        0.3482\n",
      "      9        \u001b[36m0.3081\u001b[0m        \u001b[32m0.3594\u001b[0m     +  0.3390\n",
      "     10        \u001b[36m0.3011\u001b[0m        0.3607        0.3734\n",
      "     11        \u001b[36m0.3004\u001b[0m        \u001b[32m0.3480\u001b[0m     +  0.3598\n",
      "     12        \u001b[36m0.2970\u001b[0m        \u001b[32m0.3464\u001b[0m     +  0.3416\n",
      "     13        \u001b[36m0.2944\u001b[0m        0.3598        0.3330\n",
      "     14        \u001b[36m0.2935\u001b[0m        0.3694        0.3367\n",
      "     15        0.3005        \u001b[32m0.3371\u001b[0m     +  0.3279\n",
      "     16        0.2958        0.3460        0.3402\n",
      "     17        \u001b[36m0.2898\u001b[0m        0.3624        0.3438\n",
      "     18        \u001b[36m0.2885\u001b[0m        0.3438        0.3443\n",
      "     19        0.2906        0.3416        0.3411\n",
      "     20        \u001b[36m0.2868\u001b[0m        \u001b[32m0.3323\u001b[0m     +  0.3547\n",
      "     21        \u001b[36m0.2868\u001b[0m        \u001b[32m0.3250\u001b[0m     +  0.3199\n",
      "     22        0.2900        0.3353        0.3432\n",
      "     23        0.2932        0.3278        0.3389\n",
      "     24        \u001b[36m0.2862\u001b[0m        0.3322        0.3452\n",
      "     25        \u001b[36m0.2859\u001b[0m        \u001b[32m0.3133\u001b[0m     +  0.3578\n",
      "     26        \u001b[36m0.2828\u001b[0m        0.3473        0.3669\n",
      "     27        \u001b[36m0.2785\u001b[0m        0.3235        0.3400\n",
      "     28        0.2823        0.3643        0.3350\n",
      "     29        0.2824        0.3551        0.3385\n",
      "     30        \u001b[36m0.2755\u001b[0m        0.3418        0.3430\n",
      "     31        0.2856        0.3913        0.3428\n",
      "     32        0.2797        0.4394        0.3594\n",
      "     33        \u001b[36m0.2739\u001b[0m        0.4416        0.3673\n",
      "     34        \u001b[36m0.2683\u001b[0m        0.4478        0.3264\n",
      "     35        0.2700        0.4277        0.3512\n",
      "     36        0.2688        0.4790        0.3737\n",
      "     37        0.2707        0.5122        0.3511\n",
      "     38        \u001b[36m0.2651\u001b[0m        0.5333        0.3372\n",
      "     39        0.2675        0.5708        0.3325\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.6538\u001b[0m        \u001b[32m0.5929\u001b[0m     +  0.3394\n",
      "      2        \u001b[36m0.6241\u001b[0m        0.6196        0.3424\n",
      "      3        \u001b[36m0.6095\u001b[0m        \u001b[32m0.5798\u001b[0m     +  0.3644\n",
      "      4        \u001b[36m0.6014\u001b[0m        0.5820        0.3426\n",
      "      5        \u001b[36m0.5907\u001b[0m        0.5816        0.3454\n",
      "      6        \u001b[36m0.5867\u001b[0m        0.5871        0.3329\n",
      "      7        \u001b[36m0.5856\u001b[0m        0.6012        0.3336\n",
      "      8        \u001b[36m0.5840\u001b[0m        0.5843        0.3185\n",
      "      9        \u001b[36m0.5832\u001b[0m        0.5898        0.3340\n",
      "     10        \u001b[36m0.5788\u001b[0m        \u001b[32m0.5782\u001b[0m     +  0.3406\n",
      "     11        0.5793        0.5832        0.3459\n",
      "     12        \u001b[36m0.5733\u001b[0m        0.5807        0.3512\n",
      "     13        \u001b[36m0.5706\u001b[0m        0.5816        0.3549\n",
      "     14        \u001b[36m0.5682\u001b[0m        0.5816        0.3841\n",
      "     15        \u001b[36m0.5678\u001b[0m        0.5826        0.3128\n",
      "     16        \u001b[36m0.5644\u001b[0m        0.5842        0.3557\n",
      "     17        0.5649        0.5857        0.3539\n",
      "     18        \u001b[36m0.5607\u001b[0m        0.5849        0.3159\n",
      "     19        \u001b[36m0.5568\u001b[0m        0.5860        0.3488\n",
      "     20        \u001b[36m0.5547\u001b[0m        0.5864        0.3476\n",
      "     21        \u001b[36m0.5541\u001b[0m        0.5885        0.3459\n",
      "     22        \u001b[36m0.5529\u001b[0m        0.5900        0.3536\n",
      "     23        0.5536        0.5928        0.3391\n",
      "     24        \u001b[36m0.5518\u001b[0m        0.5911        0.3592\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4781\u001b[0m        \u001b[32m0.4430\u001b[0m     +  0.3363\n",
      "      2        \u001b[36m0.4207\u001b[0m        \u001b[32m0.4425\u001b[0m     +  0.3259\n",
      "      3        \u001b[36m0.4071\u001b[0m        0.4758        0.3398\n",
      "      4        \u001b[36m0.3945\u001b[0m        0.4986        0.3549\n",
      "      5        \u001b[36m0.3840\u001b[0m        0.4697        0.3393\n",
      "      6        \u001b[36m0.3764\u001b[0m        0.4425        0.3536\n",
      "      7        \u001b[36m0.3716\u001b[0m        \u001b[32m0.4266\u001b[0m     +  0.3486\n",
      "      8        \u001b[36m0.3686\u001b[0m        \u001b[32m0.4180\u001b[0m     +  0.3533\n",
      "      9        \u001b[36m0.3656\u001b[0m        \u001b[32m0.4156\u001b[0m     +  0.3169\n",
      "     10        \u001b[36m0.3638\u001b[0m        \u001b[32m0.4129\u001b[0m     +  0.3405\n",
      "     11        \u001b[36m0.3618\u001b[0m        \u001b[32m0.4120\u001b[0m     +  0.3514\n",
      "     12        \u001b[36m0.3606\u001b[0m        \u001b[32m0.4051\u001b[0m     +  0.3398\n",
      "     13        \u001b[36m0.3579\u001b[0m        0.4153        0.3445\n",
      "     14        0.3580        0.4180        0.3440\n",
      "     15        0.3581        0.4428        0.3216\n",
      "     16        \u001b[36m0.3526\u001b[0m        0.4303        0.3519\n",
      "     17        0.3530        0.4173        0.3514\n",
      "     18        0.3527        0.4225        0.3379\n",
      "     19        \u001b[36m0.3512\u001b[0m        \u001b[32m0.3847\u001b[0m     +  0.3430\n",
      "     20        0.3557        \u001b[32m0.3520\u001b[0m     +  0.3585\n",
      "     21        0.3517        0.3645        0.3481\n",
      "     22        \u001b[36m0.3492\u001b[0m        0.3676        0.3677\n",
      "     23        0.3582        0.3768        0.3474\n",
      "     24        \u001b[36m0.3460\u001b[0m        0.3601        0.3498\n",
      "     25        \u001b[36m0.3419\u001b[0m        0.3626        0.3464\n",
      "     26        \u001b[36m0.3416\u001b[0m        0.3579        0.3495\n",
      "     27        0.3420        \u001b[32m0.3330\u001b[0m     +  0.3464\n",
      "     28        \u001b[36m0.3406\u001b[0m        0.3385        0.3426\n",
      "     29        \u001b[36m0.3349\u001b[0m        0.3358        0.3361\n",
      "     30        \u001b[36m0.3323\u001b[0m        0.3382        0.3460\n",
      "     31        \u001b[36m0.3296\u001b[0m        0.3374        0.3562\n",
      "     32        \u001b[36m0.3275\u001b[0m        0.3395        0.3325\n",
      "     33        \u001b[36m0.3256\u001b[0m        0.3408        0.3540\n",
      "     34        \u001b[36m0.3253\u001b[0m        0.3346        0.3323\n",
      "     35        \u001b[36m0.3236\u001b[0m        0.3356        0.3316\n",
      "     36        \u001b[36m0.3227\u001b[0m        0.3376        0.3374\n",
      "     37        \u001b[36m0.3211\u001b[0m        0.3382        0.3600\n",
      "     38        0.3217        0.3406        0.3390\n",
      "     39        \u001b[36m0.3195\u001b[0m        0.3405        0.3593\n",
      "     40        \u001b[36m0.3182\u001b[0m        0.3499        0.3458\n",
      "     41        \u001b[36m0.3169\u001b[0m        0.3539        0.3411\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4297\u001b[0m        \u001b[32m0.3928\u001b[0m     +  0.3586\n",
      "      2        \u001b[36m0.3933\u001b[0m        \u001b[32m0.3615\u001b[0m     +  0.3555\n",
      "      3        \u001b[36m0.3629\u001b[0m        \u001b[32m0.3573\u001b[0m     +  0.3409\n",
      "      4        \u001b[36m0.3355\u001b[0m        \u001b[32m0.3533\u001b[0m     +  0.3343\n",
      "      5        \u001b[36m0.3222\u001b[0m        \u001b[32m0.3509\u001b[0m     +  0.3604\n",
      "      6        \u001b[36m0.3191\u001b[0m        0.3519        0.3533\n",
      "      7        \u001b[36m0.3168\u001b[0m        0.3548        0.3650\n",
      "      8        \u001b[36m0.3097\u001b[0m        0.3527        0.3380\n",
      "      9        0.3112        0.3518        0.3538\n",
      "     10        \u001b[36m0.3072\u001b[0m        \u001b[32m0.3495\u001b[0m     +  0.3380\n",
      "     11        \u001b[36m0.3002\u001b[0m        \u001b[32m0.3493\u001b[0m     +  0.3504\n",
      "     12        \u001b[36m0.2994\u001b[0m        \u001b[32m0.3449\u001b[0m     +  0.3406\n",
      "     13        \u001b[36m0.2955\u001b[0m        \u001b[32m0.3419\u001b[0m     +  0.3493\n",
      "     14        \u001b[36m0.2947\u001b[0m        0.3460        0.3451\n",
      "     15        \u001b[36m0.2937\u001b[0m        0.3673        0.3422\n",
      "     16        0.2955        0.3571        0.3425\n",
      "     17        \u001b[36m0.2932\u001b[0m        0.3746        0.3436\n",
      "     18        \u001b[36m0.2886\u001b[0m        0.3899        0.3589\n",
      "     19        0.2912        0.3860        0.3417\n",
      "     20        0.2978        0.3812        0.3546\n",
      "     21        0.2946        0.3713        0.3405\n",
      "     22        \u001b[36m0.2881\u001b[0m        0.3902        0.3360\n",
      "     23        \u001b[36m0.2841\u001b[0m        0.4058        0.3429\n",
      "     24        \u001b[36m0.2805\u001b[0m        0.4286        0.3457\n",
      "     25        \u001b[36m0.2766\u001b[0m        0.4637        0.3475\n",
      "     26        \u001b[36m0.2747\u001b[0m        0.4525        0.3442\n",
      "     27        \u001b[36m0.2745\u001b[0m        0.4356        0.3467\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.5959\u001b[0m        \u001b[32m0.2441\u001b[0m     +  0.3494\n",
      "      2        \u001b[36m0.5522\u001b[0m        \u001b[32m0.2256\u001b[0m     +  0.3500\n",
      "      3        \u001b[36m0.5363\u001b[0m        0.2366        0.3576\n",
      "      4        0.5439        \u001b[32m0.2161\u001b[0m     +  0.3442\n",
      "      5        \u001b[36m0.5344\u001b[0m        \u001b[32m0.2080\u001b[0m     +  0.3432\n",
      "      6        0.5381        0.2314        0.3649\n",
      "      7        0.5401        0.2552        0.3522\n",
      "      8        0.5363        0.2523        0.3473\n",
      "      9        \u001b[36m0.5205\u001b[0m        0.2480        0.3417\n",
      "     10        \u001b[36m0.5177\u001b[0m        0.2406        0.3569\n",
      "     11        0.5281        0.2407        0.3350\n",
      "     12        0.5207        0.2333        0.3392\n",
      "     13        \u001b[36m0.5115\u001b[0m        0.2411        0.3548\n",
      "     14        \u001b[36m0.5100\u001b[0m        0.2421        0.3394\n",
      "     15        \u001b[36m0.4995\u001b[0m        0.2369        0.3526\n",
      "     16        \u001b[36m0.4976\u001b[0m        0.2391        0.3405\n",
      "     17        0.5023        0.2375        0.3695\n",
      "     18        0.4985        0.2568        0.3539\n",
      "     19        \u001b[36m0.4917\u001b[0m        0.2489        0.3301\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4636\u001b[0m        \u001b[32m0.3384\u001b[0m     +  0.3228\n",
      "      2        \u001b[36m0.4041\u001b[0m        \u001b[32m0.3218\u001b[0m     +  0.3299\n",
      "      3        \u001b[36m0.3409\u001b[0m        0.3284        0.3714\n",
      "      4        \u001b[36m0.3150\u001b[0m        0.3302        0.3337\n",
      "      5        \u001b[36m0.3010\u001b[0m        0.3387        0.3453\n",
      "      6        \u001b[36m0.2857\u001b[0m        0.3677        0.3351\n",
      "      7        \u001b[36m0.2786\u001b[0m        0.3615        0.3302\n",
      "      8        \u001b[36m0.2742\u001b[0m        0.3622        0.3439\n",
      "      9        0.2775        0.3480        0.3356\n",
      "     10        \u001b[36m0.2696\u001b[0m        0.3323        0.3401\n",
      "     11        \u001b[36m0.2614\u001b[0m        0.3336        0.3527\n",
      "     12        \u001b[36m0.2607\u001b[0m        0.3438        0.3393\n",
      "     13        \u001b[36m0.2561\u001b[0m        0.3556        0.3401\n",
      "     14        \u001b[36m0.2549\u001b[0m        0.3491        0.3490\n",
      "     15        0.2603        0.3395        0.3531\n",
      "     16        0.2581        0.3425        0.3456\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4294\u001b[0m        \u001b[32m0.2532\u001b[0m     +  0.3523\n",
      "      2        \u001b[36m0.3544\u001b[0m        0.3522        0.3435\n",
      "      3        \u001b[36m0.3149\u001b[0m        0.2742        0.3475\n",
      "      4        0.3241        0.2995        0.3567\n",
      "      5        0.3439        0.2958        0.3566\n",
      "      6        0.3213        0.2714        0.3745\n",
      "      7        \u001b[36m0.3088\u001b[0m        0.2688        0.3396\n",
      "      8        \u001b[36m0.2988\u001b[0m        0.2559        0.3671\n",
      "      9        \u001b[36m0.2981\u001b[0m        \u001b[32m0.2530\u001b[0m     +  0.3349\n",
      "     10        \u001b[36m0.2939\u001b[0m        \u001b[32m0.2515\u001b[0m     +  0.3516\n",
      "     11        0.2975        0.2635        0.3406\n",
      "     12        \u001b[36m0.2891\u001b[0m        \u001b[32m0.2510\u001b[0m     +  0.3534\n",
      "     13        \u001b[36m0.2871\u001b[0m        0.2518        0.3538\n",
      "     14        \u001b[36m0.2849\u001b[0m        0.2540        0.3698\n",
      "     15        0.2857        \u001b[32m0.2485\u001b[0m     +  0.3443\n",
      "     16        0.2880        0.2563        0.3489\n",
      "     17        0.2867        \u001b[32m0.2465\u001b[0m     +  0.3613\n",
      "     18        0.2903        0.2613        0.3845\n",
      "     19        0.2864        0.2746        0.3358\n",
      "     20        0.2903        0.2549        0.3431\n",
      "     21        0.2869        0.2564        0.3378\n",
      "     22        \u001b[36m0.2849\u001b[0m        0.2602        0.3561\n",
      "     23        0.2856        0.2623        0.3459\n",
      "     24        \u001b[36m0.2831\u001b[0m        0.2661        0.3537\n",
      "     25        \u001b[36m0.2773\u001b[0m        0.2618        0.3448\n",
      "     26        \u001b[36m0.2738\u001b[0m        0.2634        0.3515\n",
      "     27        \u001b[36m0.2722\u001b[0m        0.2672        0.3624\n",
      "     28        \u001b[36m0.2704\u001b[0m        0.2684        0.3396\n",
      "     29        0.2707        0.2715        0.3436\n",
      "     30        0.2707        0.2678        0.3339\n",
      "     31        \u001b[36m0.2703\u001b[0m        0.2686        0.3456\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3707\u001b[0m        \u001b[32m0.1961\u001b[0m     +  0.3546\n",
      "      2        \u001b[36m0.3255\u001b[0m        \u001b[32m0.1921\u001b[0m     +  0.3411\n",
      "      3        \u001b[36m0.3134\u001b[0m        0.1945        0.3360\n",
      "      4        \u001b[36m0.3013\u001b[0m        0.1957        0.3492\n",
      "      5        \u001b[36m0.2970\u001b[0m        0.1934        0.3486\n",
      "      6        \u001b[36m0.2866\u001b[0m        \u001b[32m0.1861\u001b[0m     +  0.3297\n",
      "      7        0.3033        0.2008        0.3417\n",
      "      8        0.2868        \u001b[32m0.1853\u001b[0m     +  0.3344\n",
      "      9        \u001b[36m0.2849\u001b[0m        0.1910        0.3604\n",
      "     10        \u001b[36m0.2753\u001b[0m        0.1927        0.3672\n",
      "     11        \u001b[36m0.2739\u001b[0m        0.1924        0.3520\n",
      "     12        0.2762        0.1955        0.3527\n",
      "     13        0.2747        0.1903        0.3518\n",
      "     14        \u001b[36m0.2650\u001b[0m        \u001b[32m0.1834\u001b[0m     +  0.3503\n",
      "     15        \u001b[36m0.2603\u001b[0m        0.1847        0.3348\n",
      "     16        \u001b[36m0.2578\u001b[0m        0.1852        0.3409\n",
      "     17        \u001b[36m0.2548\u001b[0m        0.1850        0.3584\n",
      "     18        \u001b[36m0.2545\u001b[0m        0.1874        0.3452\n",
      "     19        \u001b[36m0.2531\u001b[0m        0.1936        0.3476\n",
      "     20        \u001b[36m0.2502\u001b[0m        0.1973        0.3354\n",
      "     21        0.2508        0.2064        0.3310\n",
      "     22        \u001b[36m0.2493\u001b[0m        0.2014        0.3409\n",
      "     23        \u001b[36m0.2479\u001b[0m        0.2105        0.3558\n",
      "     24        \u001b[36m0.2429\u001b[0m        0.2117        0.3535\n",
      "     25        \u001b[36m0.2405\u001b[0m        0.2118        0.3456\n",
      "     26        \u001b[36m0.2381\u001b[0m        0.2143        0.3493\n",
      "     27        0.2421        0.2211        0.3453\n",
      "     28        \u001b[36m0.2378\u001b[0m        0.2286        0.3413\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.5623\u001b[0m        \u001b[32m0.6204\u001b[0m     +  0.3480\n",
      "      2        \u001b[36m0.5550\u001b[0m        0.6312        0.3357\n",
      "      3        \u001b[36m0.5363\u001b[0m        0.6821        0.3497\n",
      "      4        \u001b[36m0.5250\u001b[0m        0.6615        0.3387\n",
      "      5        \u001b[36m0.5137\u001b[0m        \u001b[32m0.5838\u001b[0m     +  0.3556\n",
      "      6        \u001b[36m0.5099\u001b[0m        0.5974        0.3345\n",
      "      7        \u001b[36m0.5042\u001b[0m        \u001b[32m0.5813\u001b[0m     +  0.3598\n",
      "      8        \u001b[36m0.5025\u001b[0m        0.5855        0.3311\n",
      "      9        \u001b[36m0.4986\u001b[0m        0.5830        0.3441\n",
      "     10        \u001b[36m0.4982\u001b[0m        \u001b[32m0.5689\u001b[0m     +  0.3608\n",
      "     11        \u001b[36m0.4968\u001b[0m        0.5758        0.3381\n",
      "     12        \u001b[36m0.4935\u001b[0m        0.5773        0.3488\n",
      "     13        \u001b[36m0.4916\u001b[0m        0.5729        0.3490\n",
      "     14        \u001b[36m0.4887\u001b[0m        0.5732        0.3507\n",
      "     15        0.4888        \u001b[32m0.5677\u001b[0m     +  0.3384\n",
      "     16        \u001b[36m0.4837\u001b[0m        0.5794        0.3441\n",
      "     17        0.4841        0.5924        0.3464\n",
      "     18        0.4839        0.6000        0.3313\n",
      "     19        \u001b[36m0.4833\u001b[0m        0.5869        0.3364\n",
      "     20        \u001b[36m0.4827\u001b[0m        0.7337        0.3481\n",
      "     21        0.4861        0.6004        0.3447\n",
      "     22        0.4900        0.5767        0.3469\n",
      "     23        \u001b[36m0.4740\u001b[0m        0.5806        0.3277\n",
      "     24        \u001b[36m0.4658\u001b[0m        0.5838        0.3433\n",
      "     25        \u001b[36m0.4621\u001b[0m        0.5998        0.3449\n",
      "     26        \u001b[36m0.4609\u001b[0m        0.6010        0.3786\n",
      "     27        \u001b[36m0.4583\u001b[0m        0.6371        0.3259\n",
      "     28        \u001b[36m0.4572\u001b[0m        0.6066        0.3412\n",
      "     29        0.4597        0.6171        0.3541\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4926\u001b[0m        \u001b[32m0.3965\u001b[0m     +  0.3495\n",
      "      2        \u001b[36m0.3679\u001b[0m        \u001b[32m0.2750\u001b[0m     +  0.3496\n",
      "      3        0.3702        \u001b[32m0.2648\u001b[0m     +  0.3501\n",
      "      4        \u001b[36m0.3458\u001b[0m        \u001b[32m0.2628\u001b[0m     +  0.3416\n",
      "      5        \u001b[36m0.3268\u001b[0m        \u001b[32m0.2602\u001b[0m     +  0.3496\n",
      "      6        \u001b[36m0.3205\u001b[0m        \u001b[32m0.2590\u001b[0m     +  0.3380\n",
      "      7        \u001b[36m0.3162\u001b[0m        0.2591        0.3508\n",
      "      8        \u001b[36m0.3112\u001b[0m        \u001b[32m0.2575\u001b[0m     +  0.3396\n",
      "      9        \u001b[36m0.3044\u001b[0m        \u001b[32m0.2573\u001b[0m     +  0.3530\n",
      "     10        \u001b[36m0.2983\u001b[0m        0.2597        0.3427\n",
      "     11        \u001b[36m0.2943\u001b[0m        0.2603        0.3540\n",
      "     12        \u001b[36m0.2906\u001b[0m        0.2626        0.3638\n",
      "     13        \u001b[36m0.2880\u001b[0m        0.2648        0.3473\n",
      "     14        \u001b[36m0.2861\u001b[0m        0.2665        0.3498\n",
      "     15        \u001b[36m0.2755\u001b[0m        0.2634        0.3357\n",
      "     16        0.2765        0.2636        0.3336\n",
      "     17        0.2805        0.2637        0.3366\n",
      "     18        \u001b[36m0.2698\u001b[0m        0.2643        0.3474\n",
      "     19        \u001b[36m0.2693\u001b[0m        0.2673        0.3435\n",
      "     20        0.2714        0.2644        0.3384\n",
      "     21        \u001b[36m0.2690\u001b[0m        0.2660        0.3513\n",
      "     22        \u001b[36m0.2642\u001b[0m        0.2677        0.3607\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.5006\u001b[0m        \u001b[32m0.4487\u001b[0m     +  0.3478\n",
      "      2        \u001b[36m0.4071\u001b[0m        0.4508        0.3423\n",
      "      3        \u001b[36m0.3843\u001b[0m        \u001b[32m0.3959\u001b[0m     +  0.3441\n",
      "      4        0.3880        \u001b[32m0.3938\u001b[0m     +  0.3588\n",
      "      5        0.3937        0.3959        0.3384\n",
      "      6        \u001b[36m0.3787\u001b[0m        0.3972        0.3456\n",
      "      7        \u001b[36m0.3716\u001b[0m        0.4013        0.3480\n",
      "      8        \u001b[36m0.3681\u001b[0m        0.3966        0.3484\n",
      "      9        \u001b[36m0.3641\u001b[0m        0.4014        0.3868\n",
      "     10        0.3672        0.4054        0.3390\n",
      "     11        \u001b[36m0.3603\u001b[0m        0.4132        0.3301\n",
      "     12        \u001b[36m0.3568\u001b[0m        0.3999        0.3547\n",
      "     13        \u001b[36m0.3532\u001b[0m        0.4012        0.3537\n",
      "     14        \u001b[36m0.3483\u001b[0m        0.3996        0.3256\n",
      "     15        \u001b[36m0.3469\u001b[0m        0.4006        0.3350\n",
      "     16        \u001b[36m0.3454\u001b[0m        0.4007        0.3471\n",
      "     17        \u001b[36m0.3423\u001b[0m        \u001b[32m0.3879\u001b[0m     +  0.3609\n",
      "     18        \u001b[36m0.3390\u001b[0m        \u001b[32m0.3866\u001b[0m     +  0.3360\n",
      "     19        \u001b[36m0.3369\u001b[0m        0.3882        0.3458\n",
      "     20        \u001b[36m0.3358\u001b[0m        0.3895        0.3319\n",
      "     21        \u001b[36m0.3347\u001b[0m        0.3907        0.3486\n",
      "     22        \u001b[36m0.3337\u001b[0m        0.3912        0.3415\n",
      "     23        \u001b[36m0.3324\u001b[0m        0.3930        0.3239\n",
      "     24        \u001b[36m0.3313\u001b[0m        0.3949        0.3359\n",
      "     25        \u001b[36m0.3294\u001b[0m        \u001b[32m0.3856\u001b[0m     +  0.3536\n",
      "     26        \u001b[36m0.3283\u001b[0m        \u001b[32m0.3843\u001b[0m     +  0.3526\n",
      "     27        \u001b[36m0.3274\u001b[0m        0.3850        0.3529\n",
      "     28        \u001b[36m0.3268\u001b[0m        0.3847        0.3514\n",
      "     29        \u001b[36m0.3259\u001b[0m        0.3857        0.3272\n",
      "     30        \u001b[36m0.3255\u001b[0m        0.3858        0.3449\n",
      "     31        \u001b[36m0.3248\u001b[0m        0.3866        0.3343\n",
      "     32        \u001b[36m0.3242\u001b[0m        0.3862        0.3541\n",
      "     33        \u001b[36m0.3230\u001b[0m        \u001b[32m0.3811\u001b[0m     +  0.3501\n",
      "     34        \u001b[36m0.3224\u001b[0m        0.3816        0.3461\n",
      "     35        \u001b[36m0.3218\u001b[0m        0.3818        0.3300\n",
      "     36        \u001b[36m0.3212\u001b[0m        0.3820        0.3352\n",
      "     37        \u001b[36m0.3207\u001b[0m        0.3825        0.3472\n",
      "     38        \u001b[36m0.3201\u001b[0m        0.3829        0.3487\n",
      "     39        \u001b[36m0.3197\u001b[0m        0.3832        0.3403\n",
      "     40        \u001b[36m0.3187\u001b[0m        0.3827        0.3512\n",
      "     41        \u001b[36m0.3183\u001b[0m        0.3831        0.3292\n",
      "     42        \u001b[36m0.3180\u001b[0m        0.3831        0.3508\n",
      "     43        \u001b[36m0.3177\u001b[0m        0.3830        0.3498\n",
      "     44        \u001b[36m0.3175\u001b[0m        0.3829        0.3301\n",
      "     45        \u001b[36m0.3172\u001b[0m        0.3833        0.3435\n",
      "     46        \u001b[36m0.3166\u001b[0m        0.3836        0.3427\n",
      "     47        \u001b[36m0.3164\u001b[0m        0.3835        0.3353\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3707\u001b[0m        \u001b[32m0.2044\u001b[0m     +  0.3590\n",
      "      2        \u001b[36m0.2710\u001b[0m        0.2246        0.3349\n",
      "      3        \u001b[36m0.2591\u001b[0m        \u001b[32m0.1811\u001b[0m     +  0.3407\n",
      "      4        \u001b[36m0.2226\u001b[0m        \u001b[32m0.1799\u001b[0m     +  0.3262\n",
      "      5        \u001b[36m0.1973\u001b[0m        \u001b[32m0.1725\u001b[0m     +  0.3532\n",
      "      6        0.2011        0.1827        0.3176\n",
      "      7        \u001b[36m0.1906\u001b[0m        \u001b[32m0.1688\u001b[0m     +  0.3413\n",
      "      8        0.1906        0.1689        0.3383\n",
      "      9        \u001b[36m0.1888\u001b[0m        0.1811        0.3485\n",
      "     10        \u001b[36m0.1834\u001b[0m        0.1708        0.3314\n",
      "     11        \u001b[36m0.1790\u001b[0m        0.1727        0.3404\n",
      "     12        \u001b[36m0.1775\u001b[0m        0.1720        0.3494\n",
      "     13        \u001b[36m0.1716\u001b[0m        \u001b[32m0.1650\u001b[0m     +  0.3330\n",
      "     14        0.1732        0.1772        0.3359\n",
      "     15        0.1754        0.1974        0.3434\n",
      "     16        \u001b[36m0.1697\u001b[0m        0.1710        0.3442\n",
      "     17        \u001b[36m0.1685\u001b[0m        0.1715        0.3417\n",
      "     18        0.1804        0.1765        0.3222\n",
      "     19        0.1794        0.1955        0.3379\n",
      "     20        0.1809        0.1677        0.3354\n",
      "     21        0.1819        0.1661        0.3372\n",
      "     22        \u001b[36m0.1635\u001b[0m        0.1700        0.3410\n",
      "     23        0.1637        0.1757        0.3254\n",
      "     24        \u001b[36m0.1556\u001b[0m        0.1733        0.3659\n",
      "     25        0.1559        0.1769        0.3443\n",
      "     26        \u001b[36m0.1527\u001b[0m        0.1840        0.3398\n",
      "     27        0.1552        0.1784        0.3357\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4787\u001b[0m        \u001b[32m0.3105\u001b[0m     +  0.3524\n",
      "      2        \u001b[36m0.3739\u001b[0m        \u001b[32m0.2898\u001b[0m     +  0.3339\n",
      "      3        \u001b[36m0.3667\u001b[0m        0.2969        0.3484\n",
      "      4        \u001b[36m0.3332\u001b[0m        0.2911        0.3458\n",
      "      5        \u001b[36m0.3234\u001b[0m        \u001b[32m0.2891\u001b[0m     +  0.3427\n",
      "      6        \u001b[36m0.3149\u001b[0m        \u001b[32m0.2834\u001b[0m     +  0.3453\n",
      "      7        \u001b[36m0.3112\u001b[0m        \u001b[32m0.2775\u001b[0m     +  0.3342\n",
      "      8        \u001b[36m0.3023\u001b[0m        0.2861        0.3312\n",
      "      9        0.3053        0.2996        0.3397\n",
      "     10        \u001b[36m0.2898\u001b[0m        0.3275        0.3364\n",
      "     11        0.2924        0.3487        0.3296\n",
      "     12        \u001b[36m0.2762\u001b[0m        0.3672        0.3198\n",
      "     13        \u001b[36m0.2749\u001b[0m        0.3637        0.3479\n",
      "     14        \u001b[36m0.2701\u001b[0m        0.3627        0.3399\n",
      "     15        0.2748        0.4097        0.3568\n",
      "     16        0.2744        0.4475        0.3555\n",
      "     17        \u001b[36m0.2567\u001b[0m        0.4275        0.3668\n",
      "     18        0.2649        0.4788        0.3270\n",
      "     19        \u001b[36m0.2435\u001b[0m        0.4517        0.3455\n",
      "     20        0.2482        0.4833        0.3471\n",
      "     21        \u001b[36m0.2397\u001b[0m        0.4770        0.3438\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3837\u001b[0m        \u001b[32m0.2747\u001b[0m     +  0.3233\n",
      "      2        \u001b[36m0.3424\u001b[0m        0.2851        0.3317\n",
      "      3        0.3511        \u001b[32m0.2728\u001b[0m     +  0.3464\n",
      "      4        \u001b[36m0.3199\u001b[0m        \u001b[32m0.2711\u001b[0m     +  0.3640\n",
      "      5        \u001b[36m0.3137\u001b[0m        \u001b[32m0.2647\u001b[0m     +  0.3488\n",
      "      6        \u001b[36m0.2967\u001b[0m        \u001b[32m0.2605\u001b[0m     +  0.3457\n",
      "      7        0.3044        0.2648        0.3457\n",
      "      8        \u001b[36m0.2909\u001b[0m        \u001b[32m0.2592\u001b[0m     +  0.3244\n",
      "      9        0.2918        0.2601        0.3463\n",
      "     10        \u001b[36m0.2843\u001b[0m        0.2593        0.3371\n",
      "     11        0.2849        \u001b[32m0.2587\u001b[0m     +  0.3442\n",
      "     12        \u001b[36m0.2820\u001b[0m        \u001b[32m0.2582\u001b[0m     +  0.3456\n",
      "     13        \u001b[36m0.2808\u001b[0m        \u001b[32m0.2580\u001b[0m     +  0.3439\n",
      "     14        \u001b[36m0.2778\u001b[0m        \u001b[32m0.2578\u001b[0m     +  0.3448\n",
      "     15        0.2786        0.2607        0.3499\n",
      "     16        0.2818        0.2639        0.3483\n",
      "     17        \u001b[36m0.2760\u001b[0m        0.2642        0.3421\n",
      "     18        \u001b[36m0.2744\u001b[0m        0.2640        0.3402\n",
      "     19        0.2766        0.2667        0.3617\n",
      "     20        0.2760        0.2633        0.3511\n",
      "     21        0.2791        0.2608        0.3369\n",
      "     22        0.2824        0.2694        0.3442\n",
      "     23        \u001b[36m0.2694\u001b[0m        0.2665        0.3284\n",
      "     24        \u001b[36m0.2660\u001b[0m        0.2749        0.3457\n",
      "     25        \u001b[36m0.2648\u001b[0m        0.2794        0.3557\n",
      "     26        0.2668        0.3091        0.3304\n",
      "     27        0.2737        0.2955        0.3607\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.4645\u001b[0m        \u001b[32m0.3307\u001b[0m     +  0.3477\n",
      "      2        \u001b[36m0.3668\u001b[0m        \u001b[32m0.3117\u001b[0m     +  0.3690\n",
      "      3        0.3692        0.3136        0.3340\n",
      "      4        0.3696        \u001b[32m0.3091\u001b[0m     +  0.3301\n",
      "      5        \u001b[36m0.3506\u001b[0m        0.3105        0.3372\n",
      "      6        \u001b[36m0.3437\u001b[0m        \u001b[32m0.3059\u001b[0m     +  0.3278\n",
      "      7        \u001b[36m0.3419\u001b[0m        \u001b[32m0.3044\u001b[0m     +  0.3369\n",
      "      8        0.3421        \u001b[32m0.3031\u001b[0m     +  0.3518\n",
      "      9        \u001b[36m0.3392\u001b[0m        0.3042        0.3321\n",
      "     10        \u001b[36m0.3390\u001b[0m        0.3066        0.3382\n",
      "     11        \u001b[36m0.3378\u001b[0m        0.3093        0.3412\n",
      "     12        \u001b[36m0.3339\u001b[0m        0.3083        0.3579\n",
      "     13        \u001b[36m0.3268\u001b[0m        0.3067        0.3448\n",
      "     14        \u001b[36m0.3247\u001b[0m        0.3046        0.3731\n",
      "     15        \u001b[36m0.3199\u001b[0m        0.3089        0.3519\n",
      "     16        0.3227        0.3099        0.3438\n",
      "     17        0.3253        0.3133        0.3435\n",
      "     18        0.3321        0.3096        0.3585\n",
      "     19        \u001b[36m0.3188\u001b[0m        0.3140        0.3360\n",
      "     20        \u001b[36m0.3162\u001b[0m        0.3151        0.3431\n",
      "     21        \u001b[36m0.3142\u001b[0m        0.3167        0.3442\n",
      "     22        \u001b[36m0.3087\u001b[0m        0.3153        0.3505\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3626\u001b[0m        \u001b[32m0.1750\u001b[0m     +  0.3310\n",
      "      2        \u001b[36m0.2791\u001b[0m        \u001b[32m0.1634\u001b[0m     +  0.3516\n",
      "      3        \u001b[36m0.2280\u001b[0m        \u001b[32m0.1415\u001b[0m     +  0.3374\n",
      "      4        \u001b[36m0.1806\u001b[0m        \u001b[32m0.1388\u001b[0m     +  0.3408\n",
      "      5        0.1890        0.1394        0.3457\n",
      "      6        \u001b[36m0.1768\u001b[0m        0.1401        0.3421\n",
      "      7        \u001b[36m0.1712\u001b[0m        0.1487        0.3493\n",
      "      8        \u001b[36m0.1687\u001b[0m        0.1442        0.3291\n",
      "      9        \u001b[36m0.1613\u001b[0m        0.1459        0.3289\n",
      "     10        \u001b[36m0.1590\u001b[0m        0.1462        0.3513\n",
      "     11        \u001b[36m0.1538\u001b[0m        \u001b[32m0.1383\u001b[0m     +  0.3426\n",
      "     12        0.1569        \u001b[32m0.1371\u001b[0m     +  0.3364\n",
      "     13        \u001b[36m0.1494\u001b[0m        0.1379        0.3496\n",
      "     14        \u001b[36m0.1487\u001b[0m        0.1423        0.3353\n",
      "     15        \u001b[36m0.1479\u001b[0m        0.1433        0.3403\n",
      "     16        0.1493        0.1463        0.3455\n",
      "     17        \u001b[36m0.1441\u001b[0m        \u001b[32m0.1347\u001b[0m     +  0.3233\n",
      "     18        0.1481        0.1480        0.3468\n",
      "     19        \u001b[36m0.1437\u001b[0m        0.1361        0.3539\n",
      "     20        0.1491        0.1481        0.3471\n",
      "     21        \u001b[36m0.1415\u001b[0m        0.1429        0.3520\n",
      "     22        0.1491        0.1439        0.3270\n",
      "     23        0.1421        0.1452        0.3435\n",
      "     24        0.1477        0.1675        0.3446\n",
      "     25        0.1440        0.1673        0.3311\n",
      "     26        \u001b[36m0.1363\u001b[0m        0.1599        0.3501\n",
      "     27        \u001b[36m0.1351\u001b[0m        0.1598        0.3431\n",
      "     28        0.1442        0.1564        0.3436\n",
      "     29        0.1447        0.1491        0.3562\n",
      "     30        0.1374        0.1785        0.3431\n",
      "     31        \u001b[36m0.1326\u001b[0m        0.1792        0.3445\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3703\u001b[0m        \u001b[32m0.2058\u001b[0m     +  0.3654\n",
      "      2        \u001b[36m0.2912\u001b[0m        \u001b[32m0.1976\u001b[0m     +  0.3330\n",
      "      3        \u001b[36m0.2855\u001b[0m        \u001b[32m0.1893\u001b[0m     +  0.3493\n",
      "      4        \u001b[36m0.2653\u001b[0m        0.1988        0.3490\n",
      "      5        \u001b[36m0.2544\u001b[0m        \u001b[32m0.1723\u001b[0m     +  0.3217\n",
      "      6        0.2558        0.1765        0.3393\n",
      "      7        \u001b[36m0.2442\u001b[0m        0.1947        0.3519\n",
      "      8        \u001b[36m0.2409\u001b[0m        0.1881        0.3333\n",
      "      9        \u001b[36m0.2333\u001b[0m        0.1736        0.3665\n",
      "     10        0.2340        0.1828        0.3468\n",
      "     11        \u001b[36m0.2317\u001b[0m        \u001b[32m0.1722\u001b[0m     +  0.3431\n",
      "     12        0.2407        0.1776        0.3325\n",
      "     13        0.2346        \u001b[32m0.1705\u001b[0m     +  0.3297\n",
      "     14        0.2375        0.1722        0.3431\n",
      "     15        0.2373        \u001b[32m0.1703\u001b[0m     +  0.3344\n",
      "     16        0.2340        0.1714        0.3629\n",
      "     17        0.2567        0.1704        0.3469\n",
      "     18        0.2377        0.1707        0.3313\n",
      "     19        \u001b[36m0.2225\u001b[0m        \u001b[32m0.1701\u001b[0m     +  0.3406\n",
      "     20        \u001b[36m0.2221\u001b[0m        0.1775        0.3517\n",
      "     21        \u001b[36m0.2220\u001b[0m        0.1713        0.3357\n",
      "     22        \u001b[36m0.2160\u001b[0m        0.1707        0.3422\n",
      "     23        \u001b[36m0.2155\u001b[0m        0.1710        0.3495\n",
      "     24        \u001b[36m0.2130\u001b[0m        0.1712        0.3293\n",
      "     25        0.2166        0.1708        0.3371\n",
      "     26        \u001b[36m0.2129\u001b[0m        \u001b[32m0.1697\u001b[0m     +  0.3439\n",
      "     27        \u001b[36m0.2092\u001b[0m        \u001b[32m0.1692\u001b[0m     +  0.3455\n",
      "     28        \u001b[36m0.2077\u001b[0m        0.1695        0.3356\n",
      "     29        \u001b[36m0.2066\u001b[0m        0.1697        0.3519\n",
      "     30        \u001b[36m0.2059\u001b[0m        0.1696        0.3634\n",
      "     31        \u001b[36m0.2055\u001b[0m        0.1701        0.3270\n",
      "     32        \u001b[36m0.2051\u001b[0m        0.1700        0.3450\n",
      "     33        0.2057        0.1701        0.3307\n",
      "     34        \u001b[36m0.2043\u001b[0m        \u001b[32m0.1685\u001b[0m     +  0.3643\n",
      "     35        \u001b[36m0.2023\u001b[0m        0.1686        0.3497\n",
      "     36        \u001b[36m0.2020\u001b[0m        \u001b[32m0.1684\u001b[0m     +  0.3409\n",
      "     37        \u001b[36m0.2015\u001b[0m        \u001b[32m0.1683\u001b[0m     +  0.3434\n",
      "     38        \u001b[36m0.2010\u001b[0m        \u001b[32m0.1683\u001b[0m     +  0.3359\n",
      "     39        \u001b[36m0.2007\u001b[0m        \u001b[32m0.1682\u001b[0m     +  0.3356\n",
      "     40        \u001b[36m0.1999\u001b[0m        0.1684        0.3205\n",
      "     41        \u001b[36m0.1994\u001b[0m        0.1687        0.3336\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3102\u001b[0m        \u001b[32m0.1526\u001b[0m     +  0.3445\n",
      "      2        \u001b[36m0.2425\u001b[0m        0.1538        0.3379\n",
      "      3        \u001b[36m0.2323\u001b[0m        0.1972        0.3533\n",
      "      4        \u001b[36m0.2058\u001b[0m        \u001b[32m0.1238\u001b[0m     +  0.3295\n",
      "      5        \u001b[36m0.1566\u001b[0m        \u001b[32m0.1209\u001b[0m     +  0.3431\n",
      "      6        0.1579        0.1239        0.3485\n",
      "      7        0.1735        \u001b[32m0.1193\u001b[0m     +  0.3550\n",
      "      8        \u001b[36m0.1408\u001b[0m        \u001b[32m0.1097\u001b[0m     +  0.3389\n",
      "      9        \u001b[36m0.1373\u001b[0m        \u001b[32m0.1084\u001b[0m     +  0.3496\n",
      "     10        \u001b[36m0.1354\u001b[0m        0.1198        0.3276\n",
      "     11        0.1506        0.1268        0.3355\n",
      "     12        0.1369        0.1330        0.3429\n",
      "     13        \u001b[36m0.1346\u001b[0m        0.1178        0.3357\n",
      "     14        \u001b[36m0.1343\u001b[0m        0.1197        0.3314\n",
      "     15        0.1364        0.1105        0.3421\n",
      "     16        \u001b[36m0.1278\u001b[0m        \u001b[32m0.1078\u001b[0m     +  0.3401\n",
      "     17        0.1358        \u001b[32m0.1021\u001b[0m     +  0.3468\n",
      "     18        0.1299        0.1032        0.3551\n",
      "     19        0.1330        \u001b[32m0.1008\u001b[0m     +  0.3288\n",
      "     20        0.1328        0.1044        0.3258\n",
      "     21        0.1332        0.1045        0.3602\n",
      "     22        0.1319        0.1059        0.3570\n",
      "     23        \u001b[36m0.1269\u001b[0m        0.1106        0.3410\n",
      "     24        0.1297        0.1070        0.3315\n",
      "     25        \u001b[36m0.1235\u001b[0m        0.1116        0.3363\n",
      "     26        0.1266        0.1327        0.3419\n",
      "     27        \u001b[36m0.1211\u001b[0m        0.1299        0.3555\n",
      "     28        \u001b[36m0.1150\u001b[0m        0.1251        0.3576\n",
      "     29        0.1205        0.1313        0.3631\n",
      "     30        0.1182        0.1260        0.3560\n",
      "     31        0.1204        0.1290        0.3528\n",
      "     32        0.1153        0.1372        0.3492\n",
      "     33        \u001b[36m0.1114\u001b[0m        0.1389        0.3456\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3880\u001b[0m        \u001b[32m2.5490\u001b[0m     +  0.3380\n",
      "      2        \u001b[36m0.2265\u001b[0m        2.5897        0.3418\n",
      "      3        0.2335        \u001b[32m2.4027\u001b[0m     +  0.3455\n",
      "      4        \u001b[36m0.2053\u001b[0m        \u001b[32m2.3321\u001b[0m     +  0.3526\n",
      "      5        \u001b[36m0.1870\u001b[0m        2.4104        0.3453\n",
      "      6        0.1878        \u001b[32m2.3122\u001b[0m     +  0.3409\n",
      "      7        \u001b[36m0.1789\u001b[0m        \u001b[32m2.2431\u001b[0m     +  0.3390\n",
      "      8        \u001b[36m0.1780\u001b[0m        \u001b[32m2.2129\u001b[0m     +  0.3280\n",
      "      9        0.1790        2.2727        0.3593\n",
      "     10        0.1799        2.2427        0.3491\n",
      "     11        0.1791        \u001b[32m2.0793\u001b[0m     +  0.3438\n",
      "     12        \u001b[36m0.1631\u001b[0m        \u001b[32m2.0190\u001b[0m     +  0.3257\n",
      "     13        \u001b[36m0.1617\u001b[0m        \u001b[32m1.8733\u001b[0m     +  0.3512\n",
      "     14        \u001b[36m0.1600\u001b[0m        2.0282        0.3464\n",
      "     15        \u001b[36m0.1571\u001b[0m        2.1355        0.3392\n",
      "     16        0.1585        2.1293        0.3441\n",
      "     17        \u001b[36m0.1566\u001b[0m        2.1644        0.3238\n",
      "     18        \u001b[36m0.1562\u001b[0m        2.0221        0.3464\n",
      "     19        \u001b[36m0.1558\u001b[0m        \u001b[32m1.7871\u001b[0m     +  0.3329\n",
      "     20        0.1589        \u001b[32m1.6948\u001b[0m     +  0.3380\n",
      "     21        0.1648        1.9076        0.3396\n",
      "     22        0.1560        1.9676        0.3529\n",
      "     23        \u001b[36m0.1504\u001b[0m        2.0437        0.3597\n",
      "     24        \u001b[36m0.1471\u001b[0m        1.9362        0.3469\n",
      "     25        \u001b[36m0.1470\u001b[0m        \u001b[32m1.6217\u001b[0m     +  0.3456\n",
      "     26        0.1547        \u001b[32m1.5328\u001b[0m     +  0.3384\n",
      "     27        0.1570        1.6467        0.3506\n",
      "     28        \u001b[36m0.1452\u001b[0m        1.7111        0.3579\n",
      "     29        0.1493        1.8001        0.3481\n",
      "     30        \u001b[36m0.1434\u001b[0m        \u001b[32m1.4159\u001b[0m     +  0.3463\n",
      "     31        \u001b[36m0.1424\u001b[0m        \u001b[32m1.2280\u001b[0m     +  0.3471\n",
      "     32        0.1465        \u001b[32m1.1457\u001b[0m     +  0.3264\n",
      "     33        \u001b[36m0.1399\u001b[0m        1.3118        0.3280\n",
      "     34        0.1474        1.5099        0.3521\n",
      "     35        \u001b[36m0.1399\u001b[0m        \u001b[32m1.0940\u001b[0m     +  0.3285\n",
      "     36        0.1558        1.7883        0.3481\n",
      "     37        \u001b[36m0.1352\u001b[0m        1.1722        0.3448\n",
      "     38        0.1499        1.6604        0.3322\n",
      "     39        \u001b[36m0.1327\u001b[0m        1.2075        0.3422\n",
      "     40        0.1331        1.3823        0.3429\n",
      "     41        0.1462        1.4966        0.3307\n",
      "     42        0.1418        \u001b[32m0.9605\u001b[0m     +  0.3453\n",
      "     43        0.1354        1.3573        0.3441\n",
      "     44        \u001b[36m0.1245\u001b[0m        1.2273        0.3731\n",
      "     45        0.1252        1.1526        0.3419\n",
      "     46        0.1257        1.1006        0.3580\n",
      "     47        \u001b[36m0.1229\u001b[0m        1.1879        0.3382\n",
      "     48        \u001b[36m0.1212\u001b[0m        1.2226        0.3467\n",
      "     49        0.1292        1.0705        0.3458\n",
      "     50        \u001b[36m0.1201\u001b[0m        1.2544        0.3364\n",
      "     51        \u001b[36m0.1186\u001b[0m        1.0971        0.3525\n",
      "     52        \u001b[36m0.1175\u001b[0m        1.1608        0.3355\n",
      "     53        \u001b[36m0.1148\u001b[0m        1.2087        0.3306\n",
      "     54        \u001b[36m0.1137\u001b[0m        1.1732        0.3391\n",
      "     55        0.1146        1.1222        0.3552\n",
      "     56        \u001b[36m0.1117\u001b[0m        1.1934        0.3330\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codygrogan/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.3117\u001b[0m        \u001b[32m0.2093\u001b[0m     +  0.3527\n",
      "      2        \u001b[36m0.2545\u001b[0m        \u001b[32m0.1590\u001b[0m     +  0.3392\n",
      "      3        0.2583        0.2420        0.3922\n",
      "      4        \u001b[36m0.2496\u001b[0m        0.2012        0.3549\n",
      "      5        \u001b[36m0.2260\u001b[0m        0.1828        0.3543\n",
      "      6        \u001b[36m0.2260\u001b[0m        0.1714        0.3549\n",
      "      7        \u001b[36m0.2231\u001b[0m        0.1684        0.3445\n",
      "      8        \u001b[36m0.2130\u001b[0m        0.1606        0.3447\n",
      "      9        \u001b[36m0.2087\u001b[0m        0.1691        0.3488\n",
      "     10        \u001b[36m0.2049\u001b[0m        0.1638        0.3323\n",
      "     11        \u001b[36m0.2049\u001b[0m        0.1598        0.3446\n",
      "     12        \u001b[36m0.2004\u001b[0m        \u001b[32m0.1586\u001b[0m     +  0.3498\n",
      "     13        0.2017        0.1608        0.3463\n",
      "     14        \u001b[36m0.1963\u001b[0m        \u001b[32m0.1581\u001b[0m     +  0.3662\n",
      "     15        0.1983        \u001b[32m0.1542\u001b[0m     +  0.3365\n",
      "     16        0.1967        \u001b[32m0.1534\u001b[0m     +  0.3417\n",
      "     17        \u001b[36m0.1944\u001b[0m        \u001b[32m0.1530\u001b[0m     +  0.3358\n",
      "     18        \u001b[36m0.1924\u001b[0m        0.1531        0.3430\n",
      "     19        \u001b[36m0.1921\u001b[0m        0.1537        0.3555\n",
      "     20        \u001b[36m0.1903\u001b[0m        0.1537        0.3517\n",
      "     21        \u001b[36m0.1892\u001b[0m        0.1542        0.3371\n",
      "     22        \u001b[36m0.1890\u001b[0m        0.1543        0.3541\n",
      "     23        \u001b[36m0.1866\u001b[0m        0.1552        0.3194\n",
      "     24        0.1872        0.1534        0.3488\n",
      "     25        \u001b[36m0.1850\u001b[0m        0.1535        0.3296\n",
      "     26        0.1852        0.1531        0.3633\n",
      "     27        \u001b[36m0.1824\u001b[0m        0.1535        0.3457\n",
      "     28        0.1829        0.1534        0.3443\n",
      "     29        \u001b[36m0.1812\u001b[0m        0.1536        0.3429\n",
      "     30        0.1827        0.1551        0.3438\n",
      "     31        \u001b[36m0.1810\u001b[0m        0.1544        0.3342\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    }
   ],
   "source": [
    "for cluster in df[split_col].unique():\n",
    "    train_df = train_df_by_cluster[cluster]\n",
    "    train_x = train_df[x_cols].values.astype(np.float32)\n",
    "    train_y = train_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "    if not final_run:\n",
    "        val_df = val_df_by_cluster[cluster]\n",
    "        train_params['train_split'] = predefined_split(Dataset(val_df[x_cols].values.astype(np.float32), val_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)))\n",
    "    else:\n",
    "        train_params['train_split'] = None\n",
    "\n",
    "    callbacks = [EarlyStopping(patience=15, threshold=0.001, threshold_mode='abs', monitor='valid_loss', lower_is_better=True),\n",
    "            Checkpoint(monitor='valid_loss_best', f_params=f'sales_forecaster_{cluster}.pt', dirname='models/'),\n",
    "            LRScheduler(policy=ReduceLROnPlateau, monitor='valid_loss', factor=0.5, patience=5, threshold=0.001, threshold_mode='abs', mode='min', verbose=True)\n",
    "            ]\n",
    "\n",
    "    train_params['callbacks'] = callbacks\n",
    "\n",
    "    net = NeuralNetRegressor(NeuralNetwork(**net_params), **train_params)\n",
    "\n",
    "    net.fit(train_x, train_y)\n",
    "    net_by_cluster[cluster] = net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Nets from Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in df[split_col].unique():\n",
    "    net = NeuralNetRegressor(NeuralNetwork(**net_params), **train_params)\n",
    "    net.initialize()\n",
    "    net.load_params(f_params=f'models/sales_forecaster_{cluster}.pt')\n",
    "    net_by_cluster[cluster] = net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "cluster_rfs = {}\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    train_df = train_df_by_cluster[cluster]\n",
    "\n",
    "    train_x = train_df[x_cols].values.astype(np.float32)\n",
    "    train_y = train_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=4)\n",
    "    rf.fit(train_x, train_y.squeeze())\n",
    "\n",
    "    cluster_rfs[cluster] = rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "cluster_xgb = {}\n",
    "for cluster in df[split_col].unique():\n",
    "    train_x = train_x_by_cluster[cluster]\n",
    "    train_y = train_y_by_cluster[cluster]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=1000, max_depth=12, learning_rate=0.001, random_state=42, n_jobs=2)\n",
    "    xgb_model.fit(train_x, train_y.squeeze())\n",
    "\n",
    "    cluster_xgb[cluster] = xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_train_preds = []\n",
    "rf_train_preds = []\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    train_df = train_df_by_cluster[cluster]\n",
    "\n",
    "    train_x = train_df[x_cols].values.astype(np.float32)\n",
    "    train_y = train_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "    y_scaler = scaler_y_by_cluster[cluster]\n",
    "\n",
    "    net = net_by_cluster[cluster]\n",
    "    rf = cluster_rfs[cluster]\n",
    "\n",
    "    net_preds = net.predict(train_x)\n",
    "    rf_preds = rf.predict(train_x)\n",
    "\n",
    "    train_df['sales_nn'] = net_preds\n",
    "    train_df['sales_rf'] = rf_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loss Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF RMSLE: 0.435911921206087\n",
      "NN RMSLE: 0.4095858931541443\n"
     ]
    }
   ],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_true) - np.log1p(y_pred))))\n",
    "\n",
    "rf_preds = []\n",
    "net_preds = []\n",
    "xgb_preds = []\n",
    "val_y_true = []\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    val_cluster_df = val_df_by_cluster[cluster]\n",
    "    val_x = val_cluster_df[x_cols].values.astype(np.float32)\n",
    "    val_y = val_cluster_df[y_cols].values.reshape(-1, len(y_cols)).astype(np.float32)\n",
    "\n",
    "    rf = cluster_rfs[cluster]\n",
    "    net = net_by_cluster[cluster]\n",
    "\n",
    "    rf_preds.append(scaler_y_by_cluster[cluster].inverse_transform(rf.predict(val_x).reshape(-1, 1)))\n",
    "    net_preds.append(scaler_y_by_cluster[cluster].inverse_transform(net.predict(val_x).reshape(-1, 1)).clip(0))    \n",
    "    #xgb_preds.append(scaler_y_by_cluster[cluster].inverse_transform(cluster_xgb[cluster].predict(val_x).reshape(-1, 1)))\n",
    "    val_y_true.append(scaler_y_by_cluster[cluster].inverse_transform(val_y))\n",
    "\n",
    "rf_preds = np.concatenate(rf_preds)\n",
    "net_preds = np.concatenate(net_preds)\n",
    "#xgb_preds = np.concatenate(xgb_preds)\n",
    "val_y_true = np.concatenate(val_y_true)\n",
    "\n",
    "print(f'RF RMSLE: {rmsle(val_y_true, rf_preds)}')\n",
    "#print(f'XGB RMSLE: {rmsle(val_y_true, xgb_preds)}')\n",
    "print(f'NN RMSLE: {rmsle(val_y_true, net_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "store_nbr",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "family",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "onpromotion",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "city",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "state",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "store_type",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cluster",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "oil",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "h_type",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_locale",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "h_locale_name",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "transferred",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "holiday_count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "month",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dow_avg_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_7_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_14_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_30_sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dow_avg_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_7_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_14_transactions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rolling_30_transactions",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1bb27a0d-6c5e-49c8-9a95-db9e72a9a783",
       "rows": [
        [
         "3000888",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "3.5313807531380754",
         "3.4285714285714284",
         "3.928571428571429",
         "4.133333333333334",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ],
        [
         "3000889",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ],
        [
         "3000890",
         "1",
         "2",
         "2",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "2.6150627615062763",
         "3.857142857142857",
         "4.428571428571429",
         "3.7",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ],
        [
         "3000891",
         "1",
         "3",
         "20",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "1845.4853556485357",
         "2456.1428571428573",
         "2471.5714285714284",
         "2504.4666666666667",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ],
        [
         "3000892",
         "1",
         "4",
         "0",
         "0",
         "0",
         "0",
         "13",
         "46.8",
         "1",
         "1",
         "1",
         "1",
         null,
         "2017",
         "8",
         "16",
         "2",
         "0.1673640167364016",
         "0.0",
         "0.5714285714285714",
         "0.7333333333333333",
         "1863.3933054393303",
         "1888.857142857143",
         "1863.857142857143",
         "1859.3"
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>store_type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>oil</th>\n",
       "      <th>h_type</th>\n",
       "      <th>h_locale</th>\n",
       "      <th>...</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>dow_avg_sales</th>\n",
       "      <th>rolling_7_sales</th>\n",
       "      <th>rolling_14_sales</th>\n",
       "      <th>rolling_30_sales</th>\n",
       "      <th>dow_avg_transactions</th>\n",
       "      <th>rolling_7_transactions</th>\n",
       "      <th>rolling_14_transactions</th>\n",
       "      <th>rolling_30_transactions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3000888</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>3.531381</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000890</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2.615063</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000891</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1845.485356</td>\n",
       "      <td>2456.142857</td>\n",
       "      <td>2471.571429</td>\n",
       "      <td>2504.466667</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000892</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.167364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1863.393305</td>\n",
       "      <td>1888.857143</td>\n",
       "      <td>1863.857143</td>\n",
       "      <td>1859.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         store_nbr  family  onpromotion  city  state  store_type  cluster  \\\n",
       "id                                                                          \n",
       "3000888          1       0            0     0      0           0       13   \n",
       "3000889          1       1            0     0      0           0       13   \n",
       "3000890          1       2            2     0      0           0       13   \n",
       "3000891          1       3           20     0      0           0       13   \n",
       "3000892          1       4            0     0      0           0       13   \n",
       "\n",
       "          oil  h_type  h_locale  ...  day  day_of_week  dow_avg_sales  \\\n",
       "id                               ...                                    \n",
       "3000888  46.8       1         1  ...   16            2       3.531381   \n",
       "3000889  46.8       1         1  ...   16            2       0.000000   \n",
       "3000890  46.8       1         1  ...   16            2       2.615063   \n",
       "3000891  46.8       1         1  ...   16            2    1845.485356   \n",
       "3000892  46.8       1         1  ...   16            2       0.167364   \n",
       "\n",
       "         rolling_7_sales  rolling_14_sales  rolling_30_sales  \\\n",
       "id                                                             \n",
       "3000888         3.428571          3.928571          4.133333   \n",
       "3000889         0.000000          0.000000          0.000000   \n",
       "3000890         3.857143          4.428571          3.700000   \n",
       "3000891      2456.142857       2471.571429       2504.466667   \n",
       "3000892         0.000000          0.571429          0.733333   \n",
       "\n",
       "         dow_avg_transactions  rolling_7_transactions  \\\n",
       "id                                                      \n",
       "3000888           1863.393305             1888.857143   \n",
       "3000889           1863.393305             1888.857143   \n",
       "3000890           1863.393305             1888.857143   \n",
       "3000891           1863.393305             1888.857143   \n",
       "3000892           1863.393305             1888.857143   \n",
       "\n",
       "         rolling_14_transactions  rolling_30_transactions  \n",
       "id                                                         \n",
       "3000888              1863.857143                   1859.3  \n",
       "3000889              1863.857143                   1859.3  \n",
       "3000890              1863.857143                   1859.3  \n",
       "3000891              1863.857143                   1859.3  \n",
       "3000892              1863.857143                   1859.3  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sales",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "179f5508-6041-4eb4-a348-88e57feda975",
       "rows": [
        [
         "3000888",
         "3.8044147"
        ],
        [
         "3000889",
         "0.0005653285"
        ],
        [
         "3000890",
         "5.2990756"
        ],
        [
         "3000891",
         "2225.0566"
        ],
        [
         "3000892",
         "5.9548056e-05"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3000888</th>\n",
       "      <td>3.804415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000889</th>\n",
       "      <td>0.000565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000890</th>\n",
       "      <td>5.299076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000891</th>\n",
       "      <td>2225.056641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000892</th>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sales\n",
       "id                  \n",
       "3000888     3.804415\n",
       "3000889     0.000565\n",
       "3000890     5.299076\n",
       "3000891  2225.056641\n",
       "3000892     0.000060"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(data_dir, 'test_data.csv'), index_col=0)\n",
    "display(test_df.head())\n",
    "\n",
    "test_x_by_cluster = {}\n",
    "test_id_by_cluster = {}\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    test_cluster_min_max_scaler, test_cluster_normalize_scaler = scaler_x_by_cluster[cluster]\n",
    "    test_cluster_y_scaler = scaler_y_by_cluster[cluster]\n",
    "\n",
    "    test_cluster_x_df = test_df[test_df[split_col] == cluster]\n",
    "    test_cluster_x_df = test_cluster_x_df.drop(columns=split_col)\n",
    "\n",
    "    test_cluster_x_min_max = test_cluster_x_df[min_max_cols].values.astype(np.float32)\n",
    "    test_cluster_x_normalize = test_cluster_x_df[normalize_cols].values.astype(np.float32)\n",
    "\n",
    "    test_cluster_x_min_max = test_cluster_min_max_scaler.transform(test_cluster_x_min_max)\n",
    "    test_cluster_x_normalize = test_cluster_normalize_scaler.transform(test_cluster_x_normalize)\n",
    "\n",
    "    test_x_by_cluster[cluster] = np.concatenate([test_cluster_x_min_max, test_cluster_x_normalize], axis=1)\n",
    "    test_id_by_cluster[cluster] = test_cluster_x_df.index\n",
    "\n",
    "\n",
    "test_preds_dfs = []\n",
    "\n",
    "for cluster in df[split_col].unique():\n",
    "    test_x = test_x_by_cluster[cluster]\n",
    "    id = test_id_by_cluster[cluster]\n",
    "    #rf = cluster_rfs[cluster]\n",
    "\n",
    "    #pred_rf = scaler_y_by_cluster[cluster].inverse_transform(rf.predict(test_x).reshape(-1, 1))\n",
    "    #pred_xgb = scaler_y_by_cluster[cluster].inverse_transform(cluster_xgb[cluster].predict(test_x).reshape(-1, 1))\n",
    "    pred_nn = scaler_y_by_cluster[cluster].inverse_transform(net_by_cluster[cluster].predict(test_x).reshape(-1, 1)).clip(0)\n",
    "    \n",
    "    cluster_df = pd.DataFrame(np.concatenate([pred_nn], axis=1), index=id, columns=['sales_nn'])\n",
    "    #cluster_df = pd.DataFrame(np.concatenate([pred_rf, pred_nn], axis=1), index=id, columns=['sales_rf', 'sales_nn'])\n",
    "\n",
    "    test_preds_dfs.append(cluster_df)\n",
    "\n",
    "test_preds_df = pd.concat(test_preds_dfs)\n",
    "\n",
    "test_df = test_df.merge(test_preds_df, on='id', how='left')\n",
    "\n",
    "sub_df_nn = test_df[['sales_nn']]\n",
    "#sub_df_rf = test_df[['sales_rf']]\n",
    "#sub_df_xgb = test_df[['sales_xgb']]\n",
    "\n",
    "#sub_df_rf = sub_df_rf.rename(columns={'sales_rf': 'sales'})\n",
    "#sub_df_xgb = sub_df_xgb.rename(columns={'sales_xgb': 'sales'})\n",
    "sub_df_nn = sub_df_nn.rename(columns={'sales_nn': 'sales'})\n",
    "\n",
    "\n",
    "display(sub_df_nn.head())\n",
    "#display(sub_df_rf.head())\n",
    "#display(sub_df_xgb.head())\n",
    "\n",
    "sub_df_nn.to_csv('data/submission_nn.csv')\n",
    "#sub_df_xgb.to_csv('data/submission_xgb.csv')\n",
    "#sub_df_rf.to_csv('data/submission_rf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
